{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categoricals ands numericals in the same treatment (v2)\n",
    "> Dealing with categorical variables by encoding them, namely ordinal encoding and one-hot encoding\n",
    "- toc: true\n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: CÃ©cile Gallioz\n",
    "- categories: [sklearn, v2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = pd.read_csv(\"../../scikit-learn-mooc/datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = myData.drop(columns=\"education-num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset data contains 48842 samples and 13 features\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset data contains {myData.shape[0]} samples and {myData.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'class'\n",
    "target = myData[target_column]\n",
    "data = myData.drop(columns=target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "# \n",
    "numerical_columns = selector(dtype_exclude=object)(data)\n",
    "categorical_columns = selector(dtype_include=object)(data)\n",
    "all_columns = numerical_columns + categorical_columns\n",
    "data = data[all_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numerical = data[numerical_columns]\n",
    "data_categorical = data[categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, we know that object data type is used to represent strings and thus categorical features. Be aware that this is not always the case. Sometimes object data type could contain other types of information, such as dates that were not properly formatted (strings) and yet relate to a quantity of elapsed time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categoricals ands numericals in the same treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression + StandardScaler + OrdinalEncoder : not so good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in TRAIN is 0.803 +/- 0.001\n",
      "The accuracy in TEST  is 0.803 +/- 0.002, for 0.446 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "# \n",
    "categorical_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor, categorical_columns),\n",
    "    ('numerical', numerical_preprocessor, numerical_columns)])\n",
    "\n",
    "model = make_pipeline(preprocessor, \n",
    "                      LogisticRegression(max_iter=500))\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True)\n",
    "\n",
    "scores = cv_results[\"test_score\"]\n",
    "train_scores = cv_results[\"train_score\"]\n",
    "fit_time = cv_results[\"fit_time\"]\n",
    "\n",
    "print(\"The accuracy in TRAIN is \"\n",
    "      f\"{train_scores.mean():.3f} +/- {train_scores.std():.3f}\")\n",
    "print(\"The accuracy in TEST  is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression + StandardScaler + OneHotEncoder : good\n",
    "Linear models are nice because they are usually cheap to train, small to deploy, fast to predict and give a good baseline.\n",
    "\n",
    "However, it is often useful to check whether more complex models such as an ensemble of decision trees can lead to higher predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in TRAIN is 0.852 +/- 0.001\n",
      "The accuracy in TEST  is 0.853 +/- 0.001, for 0.836 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# \n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor, categorical_columns),\n",
    "    ('numerical', numerical_preprocessor, numerical_columns)])\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegression(max_iter=500))\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True)\n",
    "\n",
    "scores = cv_results[\"test_score\"]\n",
    "train_scores = cv_results[\"train_score\"]\n",
    "fit_time = cv_results[\"fit_time\"]\n",
    "\n",
    "print(\"The accuracy in TRAIN is \"\n",
    "      f\"{train_scores.mean():.3f} +/- {train_scores.std():.3f}\")\n",
    "print(\"The accuracy in TEST  is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting model + StandardScaler + OneHotEncoder : long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in TRAIN is 0.882 +/- 0.001\n",
      "The accuracy in TEST  is 0.874 +/- 0.002, for 3.753 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# \n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor, categorical_columns),\n",
    "    ('numerical', numerical_preprocessor, numerical_columns)])\n",
    "\n",
    "model = make_pipeline(preprocessor, \n",
    "                      HistGradientBoostingClassifier())\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True)\n",
    "\n",
    "scores = cv_results[\"test_score\"]\n",
    "train_scores = cv_results[\"train_score\"]\n",
    "fit_time = cv_results[\"fit_time\"]\n",
    "\n",
    "print(\"The accuracy in TRAIN is \"\n",
    "      f\"{train_scores.mean():.3f} +/- {train_scores.std():.3f}\")\n",
    "print(\"The accuracy in TEST  is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting model + None + OneHotEncoder : still long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in TRAIN is 0.881 +/- 0.001\n",
      "The accuracy in TEST  is 0.874 +/- 0.002, for 3.409 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# \n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor, categorical_columns)],\n",
    "    remainder=\"passthrough\")\n",
    "\n",
    "model = make_pipeline(preprocessor, \n",
    "                      HistGradientBoostingClassifier())\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True)\n",
    "\n",
    "scores = cv_results[\"test_score\"]\n",
    "train_scores = cv_results[\"train_score\"]\n",
    "fit_time = cv_results[\"fit_time\"]\n",
    "\n",
    "print(\"The accuracy in TRAIN is \"\n",
    "      f\"{train_scores.mean():.3f} +/- {train_scores.std():.3f}\")\n",
    "print(\"The accuracy in TEST  is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting model + None + OrdinalEncoder : good (the best here)\n",
    "For tree-based models, the handling of numerical and categorical variables is simpler than for linear models:\n",
    "\n",
    "- we do not need to scale the numerical features\n",
    "- using an ordinal encoding for the categorical variables is fine even if the encoding results in an arbitrary ordering\n",
    "\n",
    "We can observe that we get significantly higher accuracies with the Gradient Boosting model. This is often what we observe whenever the dataset has a large number of samples and limited number of informative features (e.g. less than 1000) with a mix of numerical and categorical variables.\n",
    "\n",
    "This explains why Gradient Boosted Machines are very popular among datascience practitioners who work with tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in TRAIN is 0.882 +/- 0.001\n",
      "The accuracy in TEST  is 0.874 +/- 0.002, for 1.504 seconds\n"
     ]
    }
   ],
   "source": [
    "categorical_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor, categorical_columns)],\n",
    "    remainder=\"passthrough\")\n",
    "\n",
    "model = make_pipeline(preprocessor, \n",
    "                      HistGradientBoostingClassifier())\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True)\n",
    "\n",
    "scores = cv_results[\"test_score\"]\n",
    "train_scores = cv_results[\"train_score\"]\n",
    "fit_time = cv_results[\"fit_time\"]\n",
    "\n",
    "print(\"The accuracy in TRAIN is \"\n",
    "      f\"{train_scores.mean():.3f} +/- {train_scores.std():.3f}\")\n",
    "print(\"The accuracy in TEST  is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
