{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding of categorical variables (v2)\n",
    "> Dealing with categorical variables by encoding them, namely ordinal encoding and one-hot encoding\n",
    "- toc: true\n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: CÃ©cile Gallioz\n",
    "- categories: [sklearn, v2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = pd.read_csv(\"../../scikit-learn-mooc/datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = myData.drop(columns=\"education-num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset data contains 48842 samples and 13 features\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset data contains {myData.shape[0]} samples and {myData.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'class'\n",
    "target = myData[target_column]\n",
    "data = myData.drop(columns=target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "# \n",
    "numerical_columns = selector(dtype_exclude=object)(data)\n",
    "categorical_columns = selector(dtype_include=object)(data)\n",
    "all_columns = numerical_columns + categorical_columns\n",
    "data = data[all_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numerical = data[numerical_columns]\n",
    "data_categorical = data[categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, we know that object data type is used to represent strings and thus categorical features. Be aware that this is not always the case. Sometimes object data type could contain other types of information, such as dates that were not properly formatted (strings) and yet relate to a quantity of elapsed time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical and numerical in the same traitment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispatch columns to a specific processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# \n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "# \n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor, categorical_columns),\n",
    "    ('numerical', numerical_preprocessor, numerical_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.851 +/- 0.003, for 0.972 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "# \n",
    "model = make_pipeline(preprocessor, LogisticRegression(max_iter=500))\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=10)\n",
    "scores = cv_results[\"test_score\"]\n",
    "fit_time = cv_results[\"fit_time\"]\n",
    "print(\"The accuracy is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OrdinalEncoder + LogisticRegression = not so good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.755 +/- 0.002, for 0.336 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# \n",
    "model = make_pipeline(OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=100), \n",
    "                      LogisticRegression(max_iter=500))\n",
    "\n",
    "cv_results = cross_validate(model, data_categorical, target, cv=10)\n",
    "scores = cv_results[\"test_score\"]\n",
    "fit_time = cv_results[\"fit_time\"]\n",
    "print(\"The accuracy is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
