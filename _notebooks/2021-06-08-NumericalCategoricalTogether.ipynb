{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using numerical and categorical variables together\n",
    "> We will show how to combine preprocessing steps on numerical and categorical\n",
    "- toc: true\n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: CÃ©cile Gallioz\n",
    "- categories: [sklearn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataFrame = pd.read_csv(\"../../scikit-learn-mooc/datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataFrame = myDataFrame.drop(columns=\"education-num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_column = 'class'\n",
    "target = myDataFrame[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " <=50K    0.760718\n",
       " >50K     0.239282\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = myDataFrame.drop(columns=target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset data contains 48842 samples and 12 features\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset data contains {data.shape[0]} samples and {data.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "workclass         object\n",
       "education         object\n",
       "marital-status    object\n",
       "occupation        object\n",
       "relationship      object\n",
       "race              object\n",
       "sex               object\n",
       "capital-gain       int64\n",
       "capital-loss       int64\n",
       "hours-per-week     int64\n",
       "native-country    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = selector(dtype_exclude=object)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = selector(dtype_include=object)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = numerical_columns + categorical_columns\n",
    "data = data[all_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset data contains 48842 samples and 12 features\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset data contains {data.shape[0]} samples and {data.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numerical = data[numerical_columns]\n",
    "data_categorical = data[categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical\n",
    "## Normalization + Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_normLin = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_normLin = cross_validate(model_normLin, data_numerical, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.800 +/- 0.004, for 0.076 seconds\n"
     ]
    }
   ],
   "source": [
    "scores = cv_results_normLin[\"test_score\"]\n",
    "fit_time = cv_results_normLin[\"fit_time\"]\n",
    "print(\"The accuracy is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical\n",
    "## Hot encoding + Regression = good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_oneHotLin = make_pipeline(\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"), \n",
    "    LogisticRegression(max_iter=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results_oneHotLin = cross_validate(model_oneHotLin, data_categorical, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.833 +/- 0.003, for 0.749 seconds\n"
     ]
    }
   ],
   "source": [
    "scores = cv_results_oneHotLin[\"test_score\"]\n",
    "fit_time = cv_results_oneHotLin[\"fit_time\"]\n",
    "print(\"The accuracy is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal encoding + Regression = not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ordLin = make_pipeline(\n",
    "    OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), \n",
    "    LogisticRegression(max_iter=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_ordLin = cross_validate(model_ordLin, data_categorical, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.755 +/- 0.002, for 0.379 seconds\n"
     ]
    }
   ],
   "source": [
    "scores = cv_results_ordLin[\"test_score\"]\n",
    "fit_time = cv_results_ordLin[\"fit_time\"]\n",
    "print(\"The accuracy is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical & Categorical\n",
    "## Normalize + Hot encoding + Linear Regression = good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "    ('standard-scaler', numerical_preprocessor, numerical_columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_both = make_pipeline(\n",
    "    preprocessor, \n",
    "    LogisticRegression(max_iter=1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_both = cross_validate(model_both, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.851 +/- 0.003, for 1.153 seconds\n"
     ]
    }
   ],
   "source": [
    "scores = cv_results_both[\"test_score\"]\n",
    "fit_time = cv_results_both[\"fit_time\"]\n",
    "print(\"The accuracy is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal encoding + Gradient-boosting trees = best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_preprocessor_tree = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                          unknown_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_tree = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor_tree, categorical_columns)], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = make_pipeline(preprocessor_tree, HistGradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_tree = cross_validate(model_tree, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.874 +/- 0.003, for 1.811 seconds\n"
     ]
    }
   ],
   "source": [
    "scores = cv_results_tree[\"test_score\"]\n",
    "fit_time = cv_results_tree[\"fit_time\"]\n",
    "print(\"The accuracy is \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
