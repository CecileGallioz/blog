{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Classifier - Useful\n",
    "> How to have a first idea on the quality of your model\n",
    "\n",
    "- toc: true\n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: CÃ©cile Gallioz\n",
    "- categories: [sklearn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Dummy classifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier.predict) will give us an idea of the \"minimum\" quality we can achieve.\n",
    "\n",
    "It returns either a fixed value or the most frequent value of the training sample.\n",
    "\n",
    "The quality of its score will be used as a floor for the future estimation. The objective is to do better or much better than the idiot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataFrame = pd.read_csv(\"../../scikit-learn-mooc/datasets/penguins_classification.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adelie       151\n",
       "Gentoo       123\n",
       "Chinstrap     68\n",
       "Name: Species, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = 'Species'\n",
    "target = myDataFrame[target_column]\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we have the weight of each class, so also the minimum quality of the estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adelie       0.441520\n",
       "Gentoo       0.359649\n",
       "Chinstrap    0.198830\n",
       "Name: Species, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuation of preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Culmen Length (mm)', 'Culmen Depth (mm)'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = myDataFrame.drop(columns=target_column)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['Culmen Length (mm)', 'Culmen Depth (mm)']\n",
    "data_numeric = data[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data_numeric, \n",
    "    target, \n",
    "    #random_state=42, \n",
    "    test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dummy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior (default) same as most frequent\n",
    "The value returne is the most frequent in the training set\n",
    "\n",
    "    model = DummyClassifier(strategy='prior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyClassifier()\n",
    "model.fit(data_train, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adelie': 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.predict(data_test)\n",
    "n = a.size\n",
    "unique, counts = np.unique(a, return_counts=True)\n",
    "dict(zip(unique, counts/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression: 0.465\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.score(data_test, target_test)\n",
    "print(f\"Accuracy of logistic regression: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified\n",
    "The value return is found randomly by respecting the class distribution of the training\n",
    "\n",
    "    model = DummyClassifier(strategy='stratified', random_state= oneInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyClassifier(strategy='stratified')\n",
    "model.fit(data_train, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adelie': 0.5116279069767442,\n",
       " 'Chinstrap': 0.16279069767441862,\n",
       " 'Gentoo': 0.32558139534883723}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.predict(data_test)\n",
    "n = a.size\n",
    "unique, counts = np.unique(a, return_counts=True)\n",
    "dict(zip(unique, counts/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression: 0.337\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.score(data_test, target_test)\n",
    "print(f\"Accuracy of logistic regression: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform\n",
    "The value return is generated uniformly at random\n",
    "\n",
    "    model = DummyClassifier(strategy='uniform', random_state= oneInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyClassifier(strategy='uniform')\n",
    "model.fit(data_train, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adelie': 0.3488372093023256,\n",
       " 'Chinstrap': 0.3023255813953488,\n",
       " 'Gentoo': 0.3488372093023256}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.predict(data_test)\n",
    "n = a.size\n",
    "unique, counts = np.unique(a, return_counts=True)\n",
    "dict(zip(unique, counts/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression: 0.372\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.score(data_test, target_test)\n",
    "print(f\"Accuracy of logistic regression: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant\n",
    "Always predicts a constant label that is provided. This is useful for metrics that evaluate a non-majority class\n",
    "\n",
    "    model = DummyClassifier(strategy='constant', constant=\"oneConstant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyClassifier(strategy='constant', constant=\"Chinstrap\")\n",
    "model.fit(data_train, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Chinstrap': 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.predict(data_test)\n",
    "n = a.size\n",
    "unique, counts = np.unique(a, return_counts=True)\n",
    "dict(zip(unique, counts/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression: 0.221\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.score(data_test, target_test)\n",
    "print(f\"Accuracy of logistic regression: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The best estimation is to prior, so any model better than that is good. \n",
    "\n",
    "We thus have a floor value, then obviously the higher the score the better the estimation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
