{
  
    
        "post0": {
            "title": "DecisionTreeRegressor (v2)",
            "content": "Loading . import pandas as pd import numpy as np import matplotlib.pyplot as plt # import seaborn as sns import time . from sklearn.datasets import fetch_california_housing housing = fetch_california_housing(as_frame=True) data, target = housing.data, housing.target target *= 100 # rescale the target in k$ . print(f&quot;The dataset data contains {data.shape[0]} samples and {data.shape[1]} features&quot;) . The dataset data contains 20640 samples and 8 features . data.dtypes . MedInc float64 HouseAge float64 AveRooms float64 AveBedrms float64 Population float64 AveOccup float64 Latitude float64 Longitude float64 dtype: object . data.head() . MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude . 0 8.3252 | 41.0 | 6.984127 | 1.023810 | 322.0 | 2.555556 | 37.88 | -122.23 | . 1 8.3014 | 21.0 | 6.238137 | 0.971880 | 2401.0 | 2.109842 | 37.86 | -122.22 | . 2 7.2574 | 52.0 | 8.288136 | 1.073446 | 496.0 | 2.802260 | 37.85 | -122.24 | . 3 5.6431 | 52.0 | 5.817352 | 1.073059 | 558.0 | 2.547945 | 37.85 | -122.25 | . 4 3.8462 | 52.0 | 6.281853 | 1.081081 | 565.0 | 2.181467 | 37.85 | -122.25 | . target.head() . 0 452.6 1 358.5 2 352.1 3 341.3 4 342.2 Name: MedHouseVal, dtype: float64 . target.plot.hist(bins=20, edgecolor=&quot;black&quot;) plt.xlabel(&quot;Median House Value (k$)&quot;) _ = plt.title(&quot;Target distribution&quot;) . Treatment : DecisionTreeRegressor . from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import cross_validate # model = DecisionTreeRegressor(random_state=0) cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0) cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True) scores = cv_results[&quot;test_score&quot;] train_scores = cv_results[&quot;train_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy in TEST is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) print(&quot;The accuracy in TRAIN is &quot; f&quot;{train_scores.mean():.3f} +/- {train_scores.std():.3f}&quot;) . The accuracy in TEST is 0.605 +/- 0.014, for 0.112 seconds The accuracy in TRAIN is 1.000 +/- 0.000 . cv_results = cross_validate(model, data, target, cv=cv, scoring=&quot;r2&quot;, return_train_score=True) scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy in TEST is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy in TEST is 0.605 +/- 0.014, for 0.107 seconds . Overfitting vs. underfitting . cv_results = cross_validate(model, data, target, cv=cv, scoring=&quot;neg_mean_absolute_error&quot;, return_train_score=True) cv_results = pd.DataFrame(cv_results) scores = cv_results[&quot;test_score&quot;] train_scores = cv_results[&quot;train_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] scores = -cv_results[&quot;test_score&quot;] train_scores = -cv_results[&quot;train_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy in TEST is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) print(&quot;The accuracy in TRAIN is &quot; f&quot;{train_scores.mean():.3f} +/- {train_scores.std():.3f}&quot;) . The accuracy in TEST is 46.266 +/- 0.961, for 0.109 seconds The accuracy in TRAIN is 0.000 +/- 0.000 . cv_results = pd.DataFrame(cv_results) scores = -cv_results[&quot;test_score&quot;] scores.plot.hist(edgecolor=&quot;black&quot;) plt.xlabel(&quot;Mean absolute error (k$)&quot;) _ = plt.title(&quot;Test error distribution&quot;) .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/v2/2022/03/07/SKLv2M2OverUnderFitting.html",
            "relUrl": "/sklearn/v2/2022/03/07/SKLv2M2OverUnderFitting.html",
            "date": " ‚Ä¢ Mar 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Categoricals ands numericals in the same treatment (v2)",
            "content": "Loading . import pandas as pd import numpy as np # import matplotlib.pyplot as plt # import seaborn as sns import time . myData = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . myData = myData.drop(columns=&quot;education-num&quot;) . print(f&quot;The dataset data contains {myData.shape[0]} samples and {myData.shape[1]} features&quot;) . The dataset data contains 48842 samples and 13 features . target_column = &#39;class&#39; target = myData[target_column] data = myData.drop(columns=target_column) . from sklearn.compose import make_column_selector as selector # numerical_columns = selector(dtype_exclude=object)(data) categorical_columns = selector(dtype_include=object)(data) all_columns = numerical_columns + categorical_columns data = data[all_columns] . data_numerical = data[numerical_columns] data_categorical = data[categorical_columns] . Here, we know that object data type is used to represent strings and thus categorical features. Be aware that this is not always the case. Sometimes object data type could contain other types of information, such as dates that were not properly formatted (strings) and yet relate to a quantity of elapsed time. . Categoricals ands numericals in the same treatment . LogisticRegression + StandardScaler + OrdinalEncoder : not so good . from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OrdinalEncoder, StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline from sklearn.model_selection import cross_validate from sklearn.model_selection import ShuffleSplit # categorical_preprocessor = OrdinalEncoder(handle_unknown=&quot;use_encoded_value&quot;, unknown_value=-1) numerical_preprocessor = StandardScaler() preprocessor = ColumnTransformer([ (&#39;categorical&#39;, categorical_preprocessor, categorical_columns), (&#39;numerical&#39;, numerical_preprocessor, numerical_columns)]) model = make_pipeline(preprocessor, LogisticRegression(max_iter=500)) cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0) cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True) scores = cv_results[&quot;test_score&quot;] train_scores = cv_results[&quot;train_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy in TEST is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) print(&quot;The accuracy in TRAIN is &quot; f&quot;{train_scores.mean():.3f} +/- {train_scores.std():.3f}&quot;) . The accuracy in TEST is 0.803 +/- 0.002, for 0.465 seconds The accuracy in TRAIN is 0.803 +/- 0.001 . LogisticRegression + StandardScaler + OneHotEncoder : good . Linear models are nice because they are usually cheap to train, small to deploy, fast to predict and give a good baseline. . However, it is often useful to check whether more complex models such as an ensemble of decision trees can lead to higher predictive performance. . from sklearn.preprocessing import OneHotEncoder # categorical_preprocessor = OneHotEncoder(handle_unknown=&quot;ignore&quot;) numerical_preprocessor = StandardScaler() preprocessor = ColumnTransformer([ (&#39;categorical&#39;, categorical_preprocessor, categorical_columns), (&#39;numerical&#39;, numerical_preprocessor, numerical_columns)]) model = make_pipeline(preprocessor, LogisticRegression(max_iter=500)) cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True) scores = cv_results[&quot;test_score&quot;] train_scores = cv_results[&quot;train_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy in TEST is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) print(&quot;The accuracy in TRAIN is &quot; f&quot;{train_scores.mean():.3f} +/- {train_scores.std():.3f}&quot;) . The accuracy in TEST is 0.853 +/- 0.001, for 0.817 seconds The accuracy in TRAIN is 0.852 +/- 0.001 . Gradient Boosting model + StandardScaler + OneHotEncoder : long . from sklearn.ensemble import HistGradientBoostingClassifier # categorical_preprocessor = OneHotEncoder(handle_unknown=&quot;ignore&quot;, sparse=False) numerical_preprocessor = StandardScaler() preprocessor = ColumnTransformer([ (&#39;categorical&#39;, categorical_preprocessor, categorical_columns), (&#39;numerical&#39;, numerical_preprocessor, numerical_columns)]) model = make_pipeline(preprocessor, HistGradientBoostingClassifier()) cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True) scores = cv_results[&quot;test_score&quot;] train_scores = cv_results[&quot;train_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy in TEST is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) print(&quot;The accuracy in TRAIN is &quot; f&quot;{train_scores.mean():.3f} +/- {train_scores.std():.3f}&quot;) . The accuracy in TEST is 0.874 +/- 0.002, for 3.422 seconds The accuracy in TRAIN is 0.882 +/- 0.001 . Gradient Boosting model + None + OneHotEncoder : still long . from sklearn.ensemble import HistGradientBoostingClassifier # categorical_preprocessor = OneHotEncoder(handle_unknown=&quot;ignore&quot;, sparse=False) preprocessor = ColumnTransformer([ (&#39;categorical&#39;, categorical_preprocessor, categorical_columns)], remainder=&quot;passthrough&quot;) model = make_pipeline(preprocessor, HistGradientBoostingClassifier()) cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True) scores = cv_results[&quot;test_score&quot;] train_scores = cv_results[&quot;train_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy in TEST is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) print(&quot;The accuracy in TRAIN is &quot; f&quot;{train_scores.mean():.3f} +/- {train_scores.std():.3f}&quot;) . The accuracy in TEST is 0.874 +/- 0.002, for 3.283 seconds The accuracy in TRAIN is 0.881 +/- 0.001 . Gradient Boosting model + None + OrdinalEncoder : good (the best here) . For tree-based models, the handling of numerical and categorical variables is simpler than for linear models: . we do not need to scale the numerical features | using an ordinal encoding for the categorical variables is fine even if the encoding results in an arbitrary ordering | . We can observe that we get significantly higher accuracies with the Gradient Boosting model. This is often what we observe whenever the dataset has a large number of samples and limited number of informative features (e.g. less than 1000) with a mix of numerical and categorical variables. . This explains why Gradient Boosted Machines are very popular among datascience practitioners who work with tabular data. . categorical_preprocessor = OrdinalEncoder(handle_unknown=&quot;use_encoded_value&quot;, unknown_value=-1) preprocessor = ColumnTransformer([ (&#39;categorical&#39;, categorical_preprocessor, categorical_columns)], remainder=&quot;passthrough&quot;) model = make_pipeline(preprocessor, HistGradientBoostingClassifier()) cv_results = cross_validate(model, data, target, cv=cv, return_train_score=True) scores = cv_results[&quot;test_score&quot;] train_scores = cv_results[&quot;train_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy in TEST is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) print(&quot;The accuracy in TRAIN is &quot; f&quot;{train_scores.mean():.3f} +/- {train_scores.std():.3f}&quot;) . The accuracy in TEST is 0.874 +/- 0.002, for 1.463 seconds The accuracy in TRAIN is 0.881 +/- 0.001 .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/v2/2022/02/24/SKLv2M1C3NumericalAndCategoricalVar.html",
            "relUrl": "/sklearn/v2/2022/02/24/SKLv2M1C3NumericalAndCategoricalVar.html",
            "date": " ‚Ä¢ Feb 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Encoding of categorical variables (v2)",
            "content": "Loading . import pandas as pd import numpy as np # import matplotlib.pyplot as plt # import seaborn as sns import time . myData = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . myData = myData.drop(columns=&quot;education-num&quot;) . print(f&quot;The dataset data contains {myData.shape[0]} samples and {myData.shape[1]} features&quot;) . The dataset data contains 48842 samples and 13 features . target_column = &#39;class&#39; target = myData[target_column] data = myData.drop(columns=target_column) . from sklearn.compose import make_column_selector as selector # numerical_columns = selector(dtype_exclude=object)(data) categorical_columns = selector(dtype_include=object)(data) all_columns = numerical_columns + categorical_columns data = data[all_columns] . data_numerical = data[numerical_columns] data_categorical = data[categorical_columns] . Identify categorical variables . data_categorical . workclass education marital-status occupation relationship race sex native-country . 0 Private | 11th | Never-married | Machine-op-inspct | Own-child | Black | Male | United-States | . 1 Private | HS-grad | Married-civ-spouse | Farming-fishing | Husband | White | Male | United-States | . 2 Local-gov | Assoc-acdm | Married-civ-spouse | Protective-serv | Husband | White | Male | United-States | . 3 Private | Some-college | Married-civ-spouse | Machine-op-inspct | Husband | Black | Male | United-States | . 4 ? | Some-college | Never-married | ? | Own-child | White | Female | United-States | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 48837 Private | Assoc-acdm | Married-civ-spouse | Tech-support | Wife | White | Female | United-States | . 48838 Private | HS-grad | Married-civ-spouse | Machine-op-inspct | Husband | White | Male | United-States | . 48839 Private | HS-grad | Widowed | Adm-clerical | Unmarried | White | Female | United-States | . 48840 Private | HS-grad | Never-married | Adm-clerical | Own-child | White | Male | United-States | . 48841 Self-emp-inc | HS-grad | Married-civ-spouse | Exec-managerial | Wife | White | Female | United-States | . 48842 rows √ó 8 columns . data_categorical[&quot;native-country&quot;].value_counts() . United-States 43832 Mexico 951 ? 857 Philippines 295 Germany 206 Puerto-Rico 184 Canada 182 El-Salvador 155 India 151 Cuba 138 England 127 China 122 South 115 Jamaica 106 Italy 105 Dominican-Republic 103 Japan 92 Guatemala 88 Poland 87 Vietnam 86 Columbia 85 Haiti 75 Portugal 67 Taiwan 65 Iran 59 Greece 49 Nicaragua 49 Peru 46 Ecuador 45 France 38 Ireland 37 Hong 30 Thailand 30 Cambodia 28 Trinadad&amp;Tobago 27 Laos 23 Yugoslavia 23 Outlying-US(Guam-USVI-etc) 23 Scotland 21 Honduras 20 Hungary 19 Holand-Netherlands 1 Name: native-country, dtype: int64 . data_categorical[&quot;native-country&quot;].value_counts().sort_index() . ? 857 Cambodia 28 Canada 182 China 122 Columbia 85 Cuba 138 Dominican-Republic 103 Ecuador 45 El-Salvador 155 England 127 France 38 Germany 206 Greece 49 Guatemala 88 Haiti 75 Holand-Netherlands 1 Honduras 20 Hong 30 Hungary 19 India 151 Iran 59 Ireland 37 Italy 105 Jamaica 106 Japan 92 Laos 23 Mexico 951 Nicaragua 49 Outlying-US(Guam-USVI-etc) 23 Peru 46 Philippines 295 Poland 87 Portugal 67 Puerto-Rico 184 Scotland 21 South 115 Taiwan 65 Thailand 30 Trinadad&amp;Tobago 27 United-States 43832 Vietnam 86 Yugoslavia 23 Name: native-country, dtype: int64 . Encoding ordinal categories . Using an OrdinalEncoder will output ordinal categories. . This means that there is an order in the resulting categories (e.g. 0 &lt; 1 &lt; 2). The impact of violating this ordering assumption is really dependent on the downstream models. Linear models will be impacted by misordered categories while tree-based models will not. . OrdinalEncoder is often a good strategy with tree-based models . You can still use an OrdinalEncoder with linear models but you need to be sure that:- the original categories (before encoding) have an ordering;- the encoded categories follow the same ordering than the original categories. . categorical_column = data_categorical[[&quot;workclass&quot;]] categorical_column.value_counts() . workclass Private 33906 Self-emp-not-inc 3862 Local-gov 3136 ? 2799 State-gov 1981 Self-emp-inc 1695 Federal-gov 1432 Without-pay 21 Never-worked 10 dtype: int64 . from sklearn.preprocessing import OrdinalEncoder # encoder = OrdinalEncoder() categorical_encoded = encoder.fit_transform(categorical_column) categorical_encoded[:5] . array([[4.], [4.], [2.], [4.], [0.]]) . categorical_column[:5] . workclass . 0 Private | . 1 Private | . 2 Local-gov | . 3 Private | . 4 ? | . encoder.categories_ . [array([&#39; ?&#39;, &#39; Federal-gov&#39;, &#39; Local-gov&#39;, &#39; Never-worked&#39;, &#39; Private&#39;, &#39; Self-emp-inc&#39;, &#39; Self-emp-not-inc&#39;, &#39; State-gov&#39;, &#39; Without-pay&#39;], dtype=object)] . Encoding nominal categories (without assuming any order) . Each category (unique value) became a column; the encoding returned, for each sample, a 1 to specify which category it belongs to. . OneHotEncoder is the encoding strategy used when the downstream models are linear models . One-hot encoding categorical variables with high cardinality can cause computational inefficiency in tree-based models. Because of this, it is not recommended to use OneHotEncoder in such cases even if the original categories do not have a given order. . One category . from sklearn.preprocessing import OneHotEncoder # encoder = OneHotEncoder(sparse=False) categorical_column = data_categorical[[&quot;workclass&quot;]] categorical_encoded = encoder.fit_transform(categorical_column) categorical_encoded[:5] . array([[0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0.]]) . sparse=False is used in the OneHotEncoder for didactic purposes, namely easier visualization of the data. Sparse matrices are efficient data structures when most of your matrix elements are zero. . feature_names = encoder.get_feature_names_out(input_features=[&quot;workclass&quot;]) categorical_encoded = pd.DataFrame(categorical_encoded, columns=feature_names) categorical_encoded[:5] . workclass_ ? workclass_ Federal-gov workclass_ Local-gov workclass_ Never-worked workclass_ Private workclass_ Self-emp-inc workclass_ Self-emp-not-inc workclass_ State-gov workclass_ Without-pay . 0 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Let&#39;s apply this encoding on the full dataset . data_categorical.head() . workclass education marital-status occupation relationship race sex native-country . 0 Private | 11th | Never-married | Machine-op-inspct | Own-child | Black | Male | United-States | . 1 Private | HS-grad | Married-civ-spouse | Farming-fishing | Husband | White | Male | United-States | . 2 Local-gov | Assoc-acdm | Married-civ-spouse | Protective-serv | Husband | White | Male | United-States | . 3 Private | Some-college | Married-civ-spouse | Machine-op-inspct | Husband | Black | Male | United-States | . 4 ? | Some-college | Never-married | ? | Own-child | White | Female | United-States | . data_encoded = encoder.fit_transform(data_categorical) data_encoded[:2] . array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]) . columns_encoded = encoder.get_feature_names_out(data_categorical.columns) pd.DataFrame(data_encoded, columns=columns_encoded)[:2] . workclass_ ? workclass_ Federal-gov workclass_ Local-gov workclass_ Never-worked workclass_ Private workclass_ Self-emp-inc workclass_ Self-emp-not-inc workclass_ State-gov workclass_ Without-pay education_ 10th ... native-country_ Portugal native-country_ Puerto-Rico native-country_ Scotland native-country_ South native-country_ Taiwan native-country_ Thailand native-country_ Trinadad&amp;Tobago native-country_ United-States native-country_ Vietnam native-country_ Yugoslavia . 0 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 1 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2 rows √ó 102 columns . LogisticRegression on categorical variables . OneHotEncoder + LogisticRegression = good . from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline from sklearn.model_selection import cross_validate # model = make_pipeline(OneHotEncoder(handle_unknown=&quot;ignore&quot;), LogisticRegression(max_iter=500)) cv_results = cross_validate(model, data_categorical, target, cv=10) scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.833 +/- 0.003, for 0.669 seconds . OrdinalEncoder + LogisticRegression = not so good . model = make_pipeline(OrdinalEncoder(handle_unknown=&quot;use_encoded_value&quot;, unknown_value=100), LogisticRegression(max_iter=500)) cv_results = cross_validate(model, data_categorical, target, cv=10) scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.755 +/- 0.002, for 0.330 seconds .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/v2/2022/02/24/SKLv2M1C3EncodingCategoricalVar.html",
            "relUrl": "/sklearn/v2/2022/02/24/SKLv2M1C3EncodingCategoricalVar.html",
            "date": " ‚Ä¢ Feb 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Using crossvalidation (v2)",
            "content": "Loading . import pandas as pd import numpy as np # import matplotlib.pyplot as plt # import seaborn as sns import time . myData = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . myData = myData.drop(columns=&quot;education-num&quot;) . print(f&quot;The dataset data contains {myData.shape[0]} samples and {myData.shape[1]} features&quot;) . The dataset data contains 48842 samples and 13 features . target_column = &#39;class&#39; target = myData[target_column] data = myData.drop(columns=target_column) . from sklearn.compose import make_column_selector as selector # numerical_columns = selector(dtype_exclude=object)(data) categorical_columns = selector(dtype_include=object)(data) all_columns = numerical_columns + categorical_columns data = data[all_columns] . data_numerical = data[numerical_columns] data_categorical = data[categorical_columns] . Split data in train and test . from sklearn.model_selection import train_test_split # data_train, data_test, target_train, target_test = train_test_split( data_numerical, target, test_size=0.25) . print(f&quot;Number of samples in testing: {data_train.shape[0]} =&gt; &quot; f&quot;{data_test.shape[0] / data_numerical.shape[0] * 100:.1f}% of the&quot; f&quot; original set&quot;) . Number of samples in testing: 36631 =&gt; 25.0% of the original set . LogisticRegression without preprocessing . the data are split with cross_validate and not with train_test_split . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_validate # model = LogisticRegression() cv_results = cross_validate(model, data_numerical, target, cv=10) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] . print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.800 +/- 0.004, for 0.183 seconds . LogisticRegression with preprocessing via pipeline . Same accuracy but better fitting time . from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline # model = make_pipeline(StandardScaler(), LogisticRegression()) cv_results = cross_validate(model, data_numerical, target, cv=10) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] . print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.800 +/- 0.004, for 0.074 seconds .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/v2/2022/02/16/SKLv2M1C2UsingCrossValidation.html",
            "relUrl": "/sklearn/v2/2022/02/16/SKLv2M1C2UsingCrossValidation.html",
            "date": " ‚Ä¢ Feb 16, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Preprocessing for numerical features (v2)",
            "content": "Loading . import pandas as pd import numpy as np # import matplotlib.pyplot as plt # import seaborn as sns import time . myData = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . myData = myData.drop(columns=&quot;education-num&quot;) myData.head() . age workclass education marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country class . 0 25 | Private | 11th | Never-married | Machine-op-inspct | Own-child | Black | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 1 38 | Private | HS-grad | Married-civ-spouse | Farming-fishing | Husband | White | Male | 0 | 0 | 50 | United-States | &lt;=50K | . 2 28 | Local-gov | Assoc-acdm | Married-civ-spouse | Protective-serv | Husband | White | Male | 0 | 0 | 40 | United-States | &gt;50K | . 3 44 | Private | Some-college | Married-civ-spouse | Machine-op-inspct | Husband | Black | Male | 7688 | 0 | 40 | United-States | &gt;50K | . 4 18 | ? | Some-college | Never-married | ? | Own-child | White | Female | 0 | 0 | 30 | United-States | &lt;=50K | . print(f&quot;The dataset data contains {myData.shape[0]} samples and {myData.shape[1]} features&quot;) . The dataset data contains 48842 samples and 13 features . target_column = &#39;class&#39; target = myData[target_column] data = myData.drop(columns=target_column) . myData.dtypes . age int64 workclass object education object marital-status object occupation object relationship object race object sex object capital-gain int64 capital-loss int64 hours-per-week int64 native-country object class object dtype: object . from sklearn.compose import make_column_selector as selector # numerical_columns = selector(dtype_exclude=object)(data) categorical_columns = selector(dtype_include=object)(data) all_columns = numerical_columns + categorical_columns data = data[all_columns] . data_numerical = data[numerical_columns] data_categorical = data[categorical_columns] . print(f&quot;The dataset data contains {data.shape[0]} samples and {data.shape[1]} features&quot;) . The dataset data contains 48842 samples and 12 features . Split data in train and test . from sklearn.model_selection import train_test_split # data_train, data_test, target_train, target_test = train_test_split( data_numerical, target, random_state=42, test_size=0.25) . print(f&quot;Number of samples in testing: {data_train.shape[0]} =&gt; &quot; f&quot;{data_train.shape[0] / data_numerical.shape[0] * 100:.1f}% of the&quot; f&quot; original set&quot;) . Number of samples in testing: 36631 =&gt; 75.0% of the original set . data_train.describe() . age capital-gain capital-loss hours-per-week . count 36631.000000 | 36631.000000 | 36631.000000 | 36631.000000 | . mean 38.642352 | 1087.077721 | 89.665311 | 40.431247 | . std 13.725748 | 7522.692939 | 407.110175 | 12.423952 | . min 17.000000 | 0.000000 | 0.000000 | 1.000000 | . 25% 28.000000 | 0.000000 | 0.000000 | 40.000000 | . 50% 37.000000 | 0.000000 | 0.000000 | 40.000000 | . 75% 48.000000 | 0.000000 | 0.000000 | 45.000000 | . max 90.000000 | 99999.000000 | 4356.000000 | 99.000000 | . LogisticRegression without preprocessing . from sklearn.linear_model import LogisticRegression # model = LogisticRegression() start = time.time() model.fit(data_train, target_train); elapsed_time = time.time() - start . accuracy = model.score(data_test, target_test) model_name = model.__class__.__name__ score = model.score(data_test, target_test) . print(f&quot;The accuracy using a {model_name} is {score:.3f} &quot; f&quot;with a fitting time of {elapsed_time:.3f} seconds &quot; f&quot;in {model.n_iter_[0]} iterations&quot;) . The accuracy using a LogisticRegression is 0.807 with a fitting time of 0.168 seconds in 59 iterations . Preprocessing on the data train . Just to see what happens to the data . from sklearn.preprocessing import StandardScaler # scaler = StandardScaler() data_train_scaled = scaler.fit_transform(data_train) . data_train_scaled = pd.DataFrame(data_train_scaled, columns=data_train.columns) data_train_scaled.describe() . age capital-gain capital-loss hours-per-week . count 3.663100e+04 | 3.663100e+04 | 3.663100e+04 | 3.663100e+04 | . mean -2.273364e-16 | 3.530310e-17 | 3.840667e-17 | 1.844684e-16 | . std 1.000014e+00 | 1.000014e+00 | 1.000014e+00 | 1.000014e+00 | . min -1.576792e+00 | -1.445084e-01 | -2.202513e-01 | -3.173852e+00 | . 25% -7.753674e-01 | -1.445084e-01 | -2.202513e-01 | -3.471139e-02 | . 50% -1.196565e-01 | -1.445084e-01 | -2.202513e-01 | -3.471139e-02 | . 75% 6.817680e-01 | -1.445084e-01 | -2.202513e-01 | 3.677425e-01 | . max 3.741752e+00 | 1.314865e+01 | 1.047970e+01 | 4.714245e+00 | . LogisticRegression with preprocessing via pipeline . Same accuracy but better fitting time . from sklearn.pipeline import make_pipeline # model = make_pipeline(StandardScaler(), LogisticRegression()) start = time.time() model.fit(data_train, target_train); elapsed_time = time.time() - start# LogisticRegression without preprocessing . accuracy = model.score(data_test, target_test) model_name = model.__class__.__name__ score = model.score(data_test, target_test) . print(f&quot;The accuracy using a {model_name} is {score:.3f} &quot; f&quot;with a fitting time of {elapsed_time:.3f} seconds &quot; f&quot;in {model[-1].n_iter_[0]} iterations&quot;) . The accuracy using a Pipeline is 0.807 with a fitting time of 0.066 seconds in 12 iterations . model.named_steps . {&#39;standardscaler&#39;: StandardScaler(), &#39;logisticregression&#39;: LogisticRegression()} .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/v2/2022/02/16/SKLv2M1C2Preprocessing.html",
            "relUrl": "/sklearn/v2/2022/02/16/SKLv2M1C2Preprocessing.html",
            "date": " ‚Ä¢ Feb 16, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "First models updated with scikit-learn (v2)",
            "content": "Loading . import pandas as pd import numpy as np # import matplotlib.pyplot as plt # import seaborn as sns import time . myData = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . myData = myData.drop(columns=&quot;education-num&quot;) myData.head() . age workclass education marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country class . 0 25 | Private | 11th | Never-married | Machine-op-inspct | Own-child | Black | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 1 38 | Private | HS-grad | Married-civ-spouse | Farming-fishing | Husband | White | Male | 0 | 0 | 50 | United-States | &lt;=50K | . 2 28 | Local-gov | Assoc-acdm | Married-civ-spouse | Protective-serv | Husband | White | Male | 0 | 0 | 40 | United-States | &gt;50K | . 3 44 | Private | Some-college | Married-civ-spouse | Machine-op-inspct | Husband | Black | Male | 7688 | 0 | 40 | United-States | &gt;50K | . 4 18 | ? | Some-college | Never-married | ? | Own-child | White | Female | 0 | 0 | 30 | United-States | &lt;=50K | . print(f&quot;The dataset data contains {myData.shape[0]} samples and {myData.shape[1]} features&quot;) . The dataset data contains 48842 samples and 13 features . target_column = &#39;class&#39; target = myData[target_column] data = myData.drop(columns=target_column) . myData.dtypes . age int64 workclass object education object marital-status object occupation object relationship object race object sex object capital-gain int64 capital-loss int64 hours-per-week int64 native-country object class object dtype: object . from sklearn.compose import make_column_selector as selector # numerical_columns = selector(dtype_exclude=object)(data) categorical_columns = selector(dtype_include=object)(data) all_columns = numerical_columns + categorical_columns data = data[all_columns] . data_numerical = data[numerical_columns] data_categorical = data[categorical_columns] . print(f&quot;The dataset data contains {data.shape[0]} samples and {data.shape[1]} features&quot;) . The dataset data contains 48842 samples and 12 features . Split data in train and test . from sklearn.model_selection import train_test_split # data_train, data_test, target_train, target_test = train_test_split( data_numerical, target, random_state=42, test_size=0.25) . print(f&quot;Number of samples in testing: {data_test.shape[0]} =&gt; &quot; f&quot;{data_test.shape[0] / data_numerical.shape[0] * 100:.1f}% of the&quot; f&quot; original set&quot;) . Number of samples in testing: 12211 =&gt; 25.0% of the original set . Classification model : K-nearest neighbors . from sklearn.neighbors import KNeighborsClassifier # model = KNeighborsClassifier() start = time.time() model.fit(data_train, target_train); elapsed_time = time.time() - start . accuracy = model.score(data_test, target_test) model_name = model.__class__.__name__ score = model.score(data_test, target_test) . print(f&quot;The accuracy using a {model_name} is {score:.3f} &quot; f&quot;with a fitting time of {elapsed_time:.3f} seconds &quot;) . The accuracy using a KNeighborsClassifier is 0.812 with a fitting time of 0.055 seconds . Classification model : LogisticRegression . from sklearn.linear_model import LogisticRegression # model = LogisticRegression() start = time.time() model.fit(data_train, target_train); elapsed_time = time.time() - start . accuracy = model.score(data_test, target_test) model_name = model.__class__.__name__ score = model.score(data_test, target_test) . print(f&quot;The accuracy using a {model_name} is {score:.3f} &quot; f&quot;with a fitting time of {elapsed_time:.3f} seconds &quot;) . The accuracy using a DummyClassifier is 0.766 with a fitting time of 0.020 seconds . Classification model : DummyClassifier . from sklearn.dummy import DummyClassifier # model = DummyClassifier(strategy=&quot;constant&quot;, constant=&quot; &gt;50K&quot;) start = time.time() model.fit(data_train, target_train); elapsed_time = time.time() - start . accuracy = model.score(data_test, target_test) model_name = model.__class__.__name__ score = model.score(data_test, target_test) . print(f&quot;The accuracy using a {model_name} is {score:.3f} &quot; f&quot;with a fitting time of {elapsed_time:.3f} seconds &quot;) . The accuracy using a DummyClassifier is 0.234 with a fitting time of 0.012 seconds . and with the most frequent (&lt;=50K) . model = DummyClassifier(strategy=&quot;most_frequent&quot;) start = time.time() model.fit(data_train, target_train); elapsed_time = time.time() - start . accuracy = model.score(data_test, target_test) model_name = model.__class__.__name__ score = model.score(data_test, target_test) . print(f&quot;The accuracy using a {model_name} is {score:.3f} &quot; f&quot;with a fitting time of {elapsed_time:.3f} seconds &quot;) . The accuracy using a DummyClassifier is 0.766 with a fitting time of 0.020 seconds .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/v2/2022/02/16/SKLv2M1C2FirstModelUpdate.html",
            "relUrl": "/sklearn/v2/2022/02/16/SKLv2M1C2FirstModelUpdate.html",
            "date": " ‚Ä¢ Feb 16, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "First model with scikit-learn (v2)",
            "content": "Loading . import pandas as pd import numpy as np # import matplotlib.pyplot as plt # import seaborn as sns # import time . myData = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census-numeric.csv&quot;) . myData.head() . age capital-gain capital-loss hours-per-week class . 0 41 | 0 | 0 | 92 | &lt;=50K | . 1 48 | 0 | 0 | 40 | &lt;=50K | . 2 60 | 0 | 0 | 25 | &lt;=50K | . 3 37 | 0 | 0 | 45 | &lt;=50K | . 4 73 | 3273 | 0 | 40 | &lt;=50K | . print(f&quot;The dataset data contains {myData.shape[0]} samples and {myData.shape[1]} features&quot;) . The dataset data contains 39073 samples and 5 features . myData.dtypes . age int64 capital-gain int64 capital-loss int64 hours-per-week int64 class object dtype: object . target_column = &#39;class&#39; target = myData[target_column] . target.value_counts() . &lt;=50K 29736 &gt;50K 9337 Name: class, dtype: int64 . pie = target.value_counts(normalize=True) pie.plot(kind=&quot;pie&quot;, label=&quot;target&quot;); . data = myData.drop(columns=target_column) . data.columns . Index([&#39;age&#39;, &#39;capital-gain&#39;, &#39;capital-loss&#39;, &#39;hours-per-week&#39;], dtype=&#39;object&#39;) . Classification model : K-nearest neighbors . Train = evaluation data (!!) . from sklearn.neighbors import KNeighborsClassifier # model = KNeighborsClassifier() model.fit(data, target); . target_predicted = model.predict(data) . target_predicted[:5] . array([&#39; &gt;50K&#39;, &#39; &lt;=50K&#39;, &#39; &lt;=50K&#39;, &#39; &lt;=50K&#39;, &#39; &lt;=50K&#39;], dtype=object) . target[:5] == target_predicted[:5] . 0 False 1 True 2 True 3 True 4 True Name: class, dtype: bool . print(f&quot;Number of correct prediction: &quot; f&quot;{(target[:5] == target_predicted[:5]).sum()} / 5&quot;) . Number of correct prediction: 4 / 5 . (target == target_predicted).mean() . 0.8238169580016892 . Train-test data split . adult_census_test = pd.read_csv(&#39;../../scikit-learn-mooc/datasets/adult-census-numeric-test.csv&#39;) . target_test = adult_census_test[target_column] data_test = adult_census_test.drop(columns=target_column) . print(f&quot;The testing dataset contains {data_test.shape[0]} samples and &quot; f&quot;{data_test.shape[1]} features&quot;) . The testing dataset contains 9769 samples and 4 features . accuracy = model.score(data_test, target_test) . model_name = model.__class__.__name__ . print(f&quot;The test accuracy using a {model_name} is &quot; f&quot;{accuracy:.3f}&quot;) . The test accuracy using a KNeighborsClassifier is 0.807 .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/v2/2022/02/16/SKLv2M1C2FirstModel.html",
            "relUrl": "/sklearn/v2/2022/02/16/SKLv2M1C2FirstModel.html",
            "date": " ‚Ä¢ Feb 16, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Data set analyse (v2)",
            "content": "Preparation . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . myData = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . myData.head() . age workclass education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country class . 0 25 | Private | 11th | 7 | Never-married | Machine-op-inspct | Own-child | Black | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 1 38 | Private | HS-grad | 9 | Married-civ-spouse | Farming-fishing | Husband | White | Male | 0 | 0 | 50 | United-States | &lt;=50K | . 2 28 | Local-gov | Assoc-acdm | 12 | Married-civ-spouse | Protective-serv | Husband | White | Male | 0 | 0 | 40 | United-States | &gt;50K | . 3 44 | Private | Some-college | 10 | Married-civ-spouse | Machine-op-inspct | Husband | Black | Male | 7688 | 0 | 40 | United-States | &gt;50K | . 4 18 | ? | Some-college | 10 | Never-married | ? | Own-child | White | Female | 0 | 0 | 30 | United-States | &lt;=50K | . print(f&quot;The dataset data contains {myData.shape[0]} samples and {myData.shape[1]} features&quot;) . The dataset data contains 48842 samples and 14 features . myData[&quot;class&quot;].value_counts() . &lt;=50K 37155 &gt;50K 11687 Name: class, dtype: int64 . myData.dtypes . age int64 workclass object education object education-num int64 marital-status object occupation object relationship object race object sex object capital-gain int64 capital-loss int64 hours-per-week int64 native-country object class object dtype: object . from sklearn.compose import make_column_selector as selector # numerical_columns = selector(dtype_include=&quot;int64&quot;)(myData) categorical_columns = selector(dtype_include=&quot;object&quot;)(myData) all_columns = numerical_columns + categorical_columns myData = myData[all_columns] . print(f&quot;The dataset data contains {myData.shape[0]} samples and {myData.shape[1]} features&quot;) . The dataset data contains 48842 samples and 14 features . data_numerical = myData[numerical_columns] data_categorical = myData[categorical_columns] . Visualization . _ = myData.hist(figsize=(20, 14)) . _ = sns.pairplot(myData) . _ = sns.pairplot(myData, hue=&quot;class&quot;) . _ = sns.pairplot(myData, vars=myData) . Education and education-num seem to be linked and they are : . pd.crosstab(index=myData[&quot;education&quot;], columns=myData[&quot;education-num&quot;]) . education-num 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 . education . 10th 0 | 0 | 0 | 0 | 0 | 1389 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 11th 0 | 0 | 0 | 0 | 0 | 0 | 1812 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12th 0 | 0 | 0 | 0 | 0 | 0 | 0 | 657 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1st-4th 0 | 247 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5th-6th 0 | 0 | 509 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7th-8th 0 | 0 | 0 | 955 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9th 0 | 0 | 0 | 0 | 756 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Assoc-acdm 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1601 | 0 | 0 | 0 | 0 | . Assoc-voc 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2061 | 0 | 0 | 0 | 0 | 0 | . Bachelors 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8025 | 0 | 0 | 0 | . Doctorate 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 594 | . HS-grad 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15784 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Masters 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2657 | 0 | 0 | . Preschool 83 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Prof-school 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 834 | 0 | . Some-college 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 10878 | 0 | 0 | 0 | 0 | 0 | 0 | .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/v2/2022/02/16/SKLv2M1C1Exploration.html",
            "relUrl": "/sklearn/v2/2022/02/16/SKLv2M1C1Exploration.html",
            "date": " ‚Ä¢ Feb 16, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Linear regression",
            "content": "A linear regression model minimizes the mean squared error on the training set. This means that the parameters obtained after the fit (i.e. coef and intercept) are the optimal parameters that minimizes the mean squared error. In other words, any other choice of parameters will yield a model with a higher mean squared error on the training set. . However, the mean squared error is difficult to interpret. The mean absolute error is more intuitive since it provides an error in the same unit as the one of the target. . Preparation . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import time from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_validate from sklearn.metrics import mean_absolute_error . myDataFrame = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/penguins_regression.csv&quot;) . myDataFrame.head() . Flipper Length (mm) Body Mass (g) . 0 181.0 | 3750.0 | . 1 186.0 | 3800.0 | . 2 195.0 | 3250.0 | . 3 193.0 | 3450.0 | . 4 190.0 | 3650.0 | . feature_names = &quot;Flipper Length (mm)&quot; target_name = &quot;Body Mass (g)&quot; data, target = myDataFrame[[feature_names]], myDataFrame[target_name] . sns.scatterplot(x=data[feature_names], y=target, color=&quot;black&quot;, alpha=0.5); . sns.pairplot(myDataFrame); . corr_df = myDataFrame.corr(method=&#39;pearson&#39;) plt.figure(figsize=(8, 6)) sns.heatmap(corr_df, annot=True) plt.show() . Model : linear regression . model = LinearRegression(); . https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter . R2 coefficient of determination . The ùëÖ2 score represents the proportion of variance of the target that is explained by the independent variables in the model. The best score possible is 1 but there is no lower bound. However, a model that predicts the expected value of the target would get a score of 0. . cv_results = cross_validate(model, data, target, scoring=&#39;r2&#39;) scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The R2 is &quot; f&quot;{scores.mean():,.3f} +/- {scores.std():,.3f}, for {fit_time.mean():,.3f} seconds&quot;) . The R2 is 0.225 +/- 0.341, for 0.002 seconds . Mean squared error of linear regresion . cv_results = cross_validate(model, data, target, scoring=&#39;neg_mean_squared_error&#39;, return_train_score=True) . train_error = -cv_results[&quot;train_score&quot;] print(f&quot;Mean squared error of linear regresion model on the train set: n&quot; f&quot;{train_error.mean():,.2f} +/- {train_error.std():,.2f}&quot;) . Mean squared error of linear regresion model on the train set: 152,698.64 +/- 9,237.95 . test_error = -cv_results[&quot;test_score&quot;] print(f&quot;Mean squared error of linear regresion model on the test set: n&quot; f&quot;{test_error.mean():,.2f} +/- {test_error.std():,.2f}&quot;) . Mean squared error of linear regresion model on the test set: 173,026.93 +/- 44,622.06 . We see that the training and testing scores are closed. It indicates that our model is not overfitting. . Mean absolute percentage error . The mean absolute percentage error introduce this relative scaling. . cv_results = cross_validate(model, data, target, scoring=&#39;neg_mean_absolute_percentage_error&#39;, return_train_score=True) . train_error = -cv_results[&quot;train_score&quot;]*100 print(f&quot;Mean absolute percentage error of linear regresion model on the train set: n&quot; f&quot;{train_error.mean():,.2f} % +/- {train_error.std():,.2f}&quot;) . Mean absolute percentage error of linear regresion model on the train set: 7.75 % +/- 0.50 . test_error = -cv_results[&quot;test_score&quot;]*100 print(f&quot;Mean absolute percentage error of linear regresion model on the test set: n&quot; f&quot;{test_error.mean():,.2f} % +/- {test_error.std():,.2f}&quot;) . Mean absolute percentage error of linear regresion model on the test set: 8.21 % +/- 2.18 . Predictions . model.fit(data, target); . a = model.coef_[0] print(f&quot;Optimal first parameter is {a:,.2f}&quot;) . Optimal first parameter is 49.69 . b = model.intercept_ print(f&quot;Optimal intercept is {b:,.2f}&quot;) . Optimal intercept is -5,780.83 . predicted_target = a * data + b . predicted_target = model.predict(data) . model_error = mean_absolute_error(target, predicted_target) print(f&quot;The mean absolute error of the optimal model is {model_error:,.2f}&quot;) . The mean absolute error of the optimal model is 313.00 . A mean absolute error of 313 means that in average, our model make an error of +/- 313 grams when predicting the body mass of a penguin given its flipper length. . sns.scatterplot(x=data[feature_names], y=target, color=&quot;black&quot;, alpha=0.5) _ = plt.title(&quot;Model using LinearRegression from scikit-learn&quot;) plt.plot(data, predicted_target); .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/2021/09/07/LinearRegression.html",
            "relUrl": "/sklearn/2021/09/07/LinearRegression.html",
            "date": " ‚Ä¢ Sep 7, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Hyperparameter tuning",
            "content": "In the previous notebook, we saw that hyperparameters can affect the statistical performance of a model. In this notebook, we will show how to optimize hyperparameters using a grid-search approach. . Preparation . import pandas as pd import matplotlib.pyplot as plt import time import random from sklearn.compose import make_column_selector as selector from sklearn.model_selection import train_test_split from sklearn.preprocessing import OrdinalEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline # for the moment this line is required to import HistGradientBoostingClassifier from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.model_selection import cross_validate from sklearn.model_selection import RandomizedSearchCV from sklearn.model_selection import GridSearchCV import seaborn as sns . myDataFrame = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . myDataFrame = myDataFrame.drop(columns=&quot;education-num&quot;) . target_column = &#39;class&#39; data = myDataFrame.drop(columns=target_column) target = myDataFrame[target_column] numerical_columns = selector(dtype_exclude=object)(data) data_numerical = myDataFrame[numerical_columns] categorical_columns = selector(dtype_include=object)(data) data_categorical = myDataFrame[categorical_columns] all_columns = numerical_columns + categorical_columns data = data[all_columns] . Construction of the model with default hyperparameters . categorical_preprocessor = OrdinalEncoder(handle_unknown=&quot;use_encoded_value&quot;, unknown_value=-1) preprocessor = ColumnTransformer([ (&#39;cat-preprocessor&#39;, categorical_preprocessor, categorical_columns)], remainder=&#39;passthrough&#39;, sparse_threshold=0) . model = Pipeline([ (&quot;preprocessor&quot;, preprocessor), (&quot;classifier&quot;, HistGradientBoostingClassifier(random_state=42, max_leaf_nodes=4))]) . cv_results = cross_validate(model, data, target) scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.863 +/- 0.002, for 0.829 seconds . for parameter in model.get_params(): print(parameter) . memory steps verbose preprocessor classifier preprocessor__n_jobs preprocessor__remainder preprocessor__sparse_threshold preprocessor__transformer_weights preprocessor__transformers preprocessor__verbose preprocessor__cat-preprocessor preprocessor__cat-preprocessor__categories preprocessor__cat-preprocessor__dtype preprocessor__cat-preprocessor__handle_unknown preprocessor__cat-preprocessor__unknown_value classifier__categorical_features classifier__early_stopping classifier__l2_regularization classifier__learning_rate classifier__loss classifier__max_bins classifier__max_depth classifier__max_iter classifier__max_leaf_nodes classifier__min_samples_leaf classifier__monotonic_cst classifier__n_iter_no_change classifier__random_state classifier__scoring classifier__tol classifier__validation_fraction classifier__verbose classifier__warm_start . Search for hyperparameters with a random and cross validation . from scipy.stats import uniform from scipy.stats import loguniform param_distributions = { &#39;classifier__learning_rate&#39;: loguniform(0.0001, 5), &#39;classifier__max_leaf_nodes&#39;: uniform(1, 100)} model_random_search = RandomizedSearchCV(model, param_distributions, n_jobs=4, cv=2) . cv_results = cross_validate(model_random_search, data, target, return_estimator=True) scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(f&quot;The accuracy via cross-validation is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy via cross-validation is 0.864 +/- 0.004, for 103.672 seconds . print(f&quot;Best parameter found&quot;) for fold_idx, estimator in enumerate(cv_results[&quot;estimator&quot;]): print(f&quot; on fold #{fold_idx + 1} : {estimator.best_params_}&quot;) . Best parameter found on fold #1 : {&#39;classifier__learning_rate&#39;: 0.16309862397316285, &#39;classifier__max_leaf_nodes&#39;: 45.12999922572979} on fold #2 : {&#39;classifier__learning_rate&#39;: 0.045232362575395146, &#39;classifier__max_leaf_nodes&#39;: 69.02541921038254} on fold #3 : {&#39;classifier__learning_rate&#39;: 0.023585278516485758, &#39;classifier__max_leaf_nodes&#39;: 30.014816646691422} on fold #4 : {&#39;classifier__learning_rate&#39;: 0.030455497715999508, &#39;classifier__max_leaf_nodes&#39;: 83.92313635555773} on fold #5 : {&#39;classifier__learning_rate&#39;: 0.035781951571963115, &#39;classifier__max_leaf_nodes&#39;: 85.52313915125612} . Search for hyperparameters with a grid and cross validation . param_grid = { &#39;classifier__learning_rate&#39;: (0.05, 0.1, 0.5, 1, 5), &#39;classifier__max_leaf_nodes&#39;: (3, 10, 30, 100)} model_grid_search = GridSearchCV(model, param_grid, n_jobs=4, cv=2) . cv_results = cross_validate(model_grid_search, data, target, return_estimator=True) scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(f&quot;The accuracy via cross-validation is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy via cross-validation is 0.874 +/- 0.002, for 14.921 seconds . print(f&quot;Best parameter found&quot;) for fold_idx, estimator in enumerate(cv_results[&quot;estimator&quot;]): print(f&quot; on fold #{fold_idx + 1} : {estimator.best_params_}&quot;) . Best parameter found on fold #1 : {&#39;classifier__learning_rate&#39;: 0.1, &#39;classifier__max_leaf_nodes&#39;: 30} on fold #2 : {&#39;classifier__learning_rate&#39;: 0.1, &#39;classifier__max_leaf_nodes&#39;: 30} on fold #3 : {&#39;classifier__learning_rate&#39;: 0.1, &#39;classifier__max_leaf_nodes&#39;: 30} on fold #4 : {&#39;classifier__learning_rate&#39;: 0.1, &#39;classifier__max_leaf_nodes&#39;: 30} on fold #5 : {&#39;classifier__learning_rate&#39;: 0.1, &#39;classifier__max_leaf_nodes&#39;: 30} . Search for hyperparameters with a grid and without cross validation (bad) . Be aware that the evaluation should normally be performed in a cross-validation framework by providing model_grid_search as a model to the cross_validate function as above. . Here, we are using a single train-test split to highlight the specificities of the model_grid_search instance. . data_train, data_test, target_train, target_test = train_test_split( data, target, random_state=42) . model_grid_search.fit(data_train, target_train) print(f&quot;The best set of parameters is: &quot; f&quot;{model_grid_search.best_params_}&quot;) . The best set of parameters is: {&#39;classifier__learning_rate&#39;: 0.1, &#39;classifier__max_leaf_nodes&#39;: 30} . cv_results_grid_train = pd.DataFrame(model_grid_search.cv_results_) . column_results = [f&quot;param_{name}&quot; for name in param_grid.keys()] column_results += [ &quot;mean_test_score&quot;, &quot;std_test_score&quot;, &quot;rank_test_score&quot;] cv_results_grid_train = cv_results_grid_train[column_results] def shorten_param(param_name): if &quot;__&quot; in param_name: return param_name.rsplit(&quot;__&quot;, 1)[1] return param_name cv_results_grid_train = cv_results_grid_train.rename(shorten_param, axis=1) cv_results_grid_train . learning_rate max_leaf_nodes mean_test_score std_test_score rank_test_score . 0 0.05 | 3 | 0.827196 | 0.000214 | 16 | . 1 0.05 | 10 | 0.862029 | 0.000222 | 9 | . 2 0.05 | 30 | 0.867598 | 0.000932 | 2 | . 3 0.05 | 100 | 0.865797 | 0.001259 | 7 | . 4 0.1 | 3 | 0.853266 | 0.000515 | 13 | . 5 0.1 | 10 | 0.866425 | 0.000359 | 4 | . 6 0.1 | 30 | 0.868063 | 0.000850 | 1 | . 7 0.1 | 100 | 0.864732 | 0.000795 | 8 | . 8 0.5 | 3 | 0.865824 | 0.000952 | 5 | . 9 0.5 | 10 | 0.865824 | 0.000031 | 6 | . 10 0.5 | 30 | 0.866479 | 0.000577 | 3 | . 11 0.5 | 100 | 0.859491 | 0.001069 | 10 | . 12 1 | 3 | 0.857389 | 0.003545 | 12 | . 13 1 | 10 | 0.858863 | 0.004036 | 11 | . 14 1 | 30 | 0.851028 | 0.002707 | 14 | . 15 1 | 100 | 0.835194 | 0.004609 | 15 | . 16 5 | 3 | 0.283476 | 0.003775 | 20 | . 17 5 | 10 | 0.527527 | 0.175411 | 19 | . 18 5 | 30 | 0.638062 | 0.144696 | 18 | . 19 5 | 100 | 0.727579 | 0.055780 | 17 | . pivoted_cv_results_grid_train = cv_results_grid_train.pivot_table( values=&quot;mean_test_score&quot;, index=[&quot;learning_rate&quot;], columns=[&quot;max_leaf_nodes&quot;]) pivoted_cv_results_grid_train . max_leaf_nodes 3 10 30 100 . learning_rate . 0.05 0.827196 | 0.862029 | 0.867598 | 0.865797 | . 0.10 0.853266 | 0.866425 | 0.868063 | 0.864732 | . 0.50 0.865824 | 0.865824 | 0.866479 | 0.859491 | . 1.00 0.857389 | 0.858863 | 0.851028 | 0.835194 | . 5.00 0.283476 | 0.527527 | 0.638062 | 0.727579 | . ax = sns.heatmap(pivoted_cv_results_grid_train, annot=True, cmap=&quot;YlGnBu&quot;, vmin=0.7, vmax=0.9) ax.invert_yaxis() . The above tables highlights the following things: . for too high values of learning_rate, the statistical performance of the model is degraded and adjusting the value of max_leaf_nodes cannot fix that problem; | outside of this pathological region, we observe that the optimal choice of max_leaf_nodes depends on the value of learning_rate; | in particular, we observe a &quot;diagonal&quot; of good models with an accuracy close to the maximal of 0.87: when the value of max_leaf_nodes is increased, one should increase the value of learning_rate accordingly to preserve a good accuracy. | . For now we will note that, in general, there is no unique optimal parameter setting: 6 models out of the 16 parameter configuration reach the maximal accuracy (up to small random fluctuations caused by the sampling of the training set). .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/2021/09/03/BestHyperparameters.html",
            "relUrl": "/sklearn/2021/09/03/BestHyperparameters.html",
            "date": " ‚Ä¢ Sep 3, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Set and get hyperparameters in scikit-learn",
            "content": "This notebook shows how one can get and set the value of a hyperparameter in a scikit-learn estimator. We recall that hyperparameters refer to the parameter that will control the learning process. . They should not be confused with the fitted parameters, resulting from the training. These fitted parameters are recognizable in scikit-learn because they are spelled with a final underscore _, for instance model.coef_. . Preparation . import pandas as pd import matplotlib.pyplot as plt import time from sklearn.compose import make_column_selector as selector from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_validate . myDataFrame = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . myDataFrame = myDataFrame.drop(columns=&quot;education-num&quot;) . target_column = &#39;class&#39; data = myDataFrame.drop(columns=target_column) numerical_columns = selector(dtype_exclude=object)(data) target = myDataFrame[target_column] data_numerical = myDataFrame[numerical_columns] . data_numerical.head() . age capital-gain capital-loss hours-per-week . 0 25 | 0 | 0 | 40 | . 1 38 | 0 | 0 | 50 | . 2 28 | 0 | 0 | 40 | . 3 44 | 7688 | 0 | 40 | . 4 18 | 0 | 0 | 30 | . Simple predictive model : scaler + logistic regression . Default values . model = Pipeline(steps=[ (&quot;preprocessor&quot;, StandardScaler()), (&quot;classifier&quot;, LogisticRegression()) ]) . cv_results = cross_validate(model, data_numerical, target) scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.800 +/- 0.003, for 0.072 seconds . The list of all the parameters of the pipeline . for parameter in model.get_params(): print(parameter) . memory steps verbose preprocessor classifier preprocessor__copy preprocessor__with_mean preprocessor__with_std classifier__C classifier__class_weight classifier__dual classifier__fit_intercept classifier__intercept_scaling classifier__l1_ratio classifier__max_iter classifier__multi_class classifier__n_jobs classifier__penalty classifier__random_state classifier__solver classifier__tol classifier__verbose classifier__warm_start . Change one parameter . C : float, default=1.0 . Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. . model.set_params(classifier__C=1e-3) . Pipeline(steps=[(&#39;preprocessor&#39;, StandardScaler()), (&#39;classifier&#39;, LogisticRegression(C=0.001))]) . cv_results = cross_validate(model, data_numerical, target) scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.787 +/- 0.002, for 0.066 seconds . model.get_params()[&#39;classifier__C&#39;] . 0.001 . Search a good value . for C in [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]: model.set_params(classifier__C=C) cv_results = cross_validate(model, data_numerical, target) scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(f&quot;The accuracy via cross-validation with C={C} is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy via cross-validation with C=0.0001 is 0.766 +/- 0.001, for 0.057 seconds The accuracy via cross-validation with C=0.001 is 0.787 +/- 0.002, for 0.061 seconds The accuracy via cross-validation with C=0.01 is 0.799 +/- 0.003, for 0.068 seconds The accuracy via cross-validation with C=0.1 is 0.800 +/- 0.003, for 0.072 seconds The accuracy via cross-validation with C=1 is 0.800 +/- 0.003, for 0.068 seconds The accuracy via cross-validation with C=10 is 0.800 +/- 0.003, for 0.069 seconds The accuracy via cross-validation with C=100 is 0.800 +/- 0.003, for 0.067 seconds .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/2021/08/03/SetGetHyperparameters.html",
            "relUrl": "/sklearn/2021/08/03/SetGetHyperparameters.html",
            "date": " ‚Ä¢ Aug 3, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Under and over fitting",
            "content": "Underfitting vs. Overfitting - Actual vs estimated function . scikit-learn documentation . This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. . The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. . We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting. . A polynomial of degree 4 approximates the true function almost perfectly. . However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data. . We evaluate quantitatively overfitting / underfitting by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data. . import numpy as np import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score . def true_fun(X): return np.cos(1.5 * np.pi * X) np.random.seed(0) n_samples = 50 degrees = [1, 4, 15] X = np.sort(np.random.rand(n_samples)) y = true_fun(X) + np.random.randn(n_samples) * 0.1 . plt.figure(figsize=(15, 5)) for i in range(len(degrees)): ax = plt.subplot(1, len(degrees), i + 1) plt.setp(ax, xticks=(), yticks=()) polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = LinearRegression() pipeline = Pipeline([(&quot;polynomial_features&quot;, polynomial_features), (&quot;linear_regression&quot;, linear_regression)]) pipeline.fit(X[:, np.newaxis], y) # Evaluate the models using crossvalidation scores = cross_val_score(pipeline, X[:, np.newaxis], y, scoring=&quot;neg_mean_squared_error&quot;, cv=10) X_test = np.linspace(0, 1, 100) plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=&quot;Model&quot;) plt.plot(X_test, true_fun(X_test), label=&quot;True function&quot;) plt.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.xlim((0, 1)) plt.ylim((-2, 2)) plt.legend(loc=&quot;best&quot;) plt.title(&quot;Degree {} nMSE {:.2e}(+/- {:.2e})&quot;.format( degrees[i], -scores.mean(), scores.std())) plt.show() . Underfitting vs. Overfitting - Train vs test error . Preparation . import pandas as pd import numpy as np import matplotlib.pyplot as plt import time from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import cross_validate from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import validation_curve from sklearn.model_selection import learning_curve . from sklearn.datasets import fetch_california_housing myDataFrame = fetch_california_housing(as_frame=True) . data, target = myDataFrame.data, myDataFrame.target target *= 100 # rescale the target in k$ . print(f&quot;The dataset data contains {data.shape[0]} samples and {data.shape[1]} features&quot;) . The dataset data contains 20640 samples and 8 features . data.dtypes . MedInc float64 HouseAge float64 AveRooms float64 AveBedrms float64 Population float64 AveOccup float64 Latitude float64 Longitude float64 dtype: object . Validation curve . regressor = DecisionTreeRegressor() . cv = ShuffleSplit(n_splits=30, test_size=0.2) . cv_results = cross_validate(regressor, data, target, cv=cv, scoring=&quot;neg_mean_absolute_error&quot;, return_train_score=True, n_jobs=2) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is -46.088 +/- 0.886, for 0.135 seconds . cv_results = pd.DataFrame(cv_results) . scores = pd.DataFrame() . scores[[&quot;train error&quot;, &quot;test error&quot;]] = -cv_results[ [&quot;train_score&quot;, &quot;test_score&quot;]] . scores.plot.hist(bins=50, edgecolor=&quot;black&quot;, density=True) plt.xlabel(&quot;Mean absolute error (k$)&quot;) _ = plt.title(&quot;Train and test errors distribution via cross-validation&quot;) . Here, we observe a small training error (actually zero), meaning that the model is not under-fitting: it is flexible enough to capture any variations present in the training set. . However the significantly larger testing error tells us that the model is over-fitting: the model has memorized many variations of the training set that could be considered &quot;noisy&quot; because they do not generalize to help us make good prediction on the test set. . %%time max_depth = [1, 5, 10, 15, 20, 25] train_scores, test_scores = validation_curve( regressor, data, target, param_name=&quot;max_depth&quot;, param_range=max_depth, cv=cv, scoring=&quot;neg_mean_absolute_error&quot;, n_jobs=2) train_errors, test_errors = -train_scores, -test_scores . Wall time: 8.67 s . plt.plot(max_depth, train_errors.mean(axis=1), label=&quot;Training error&quot;) plt.plot(max_depth, test_errors.mean(axis=1), label=&quot;Testing error&quot;) plt.legend() plt.xlabel(&quot;Maximum depth of decision tree&quot;) plt.ylabel(&quot;Mean absolute error (k$)&quot;) _ = plt.title(&quot;Validation curve for decision tree&quot;) . plt.errorbar(max_depth, train_errors.mean(axis=1), yerr=train_errors.std(axis=1), label=&#39;Training error&#39;) plt.errorbar(max_depth, test_errors.mean(axis=1), yerr=test_errors.std(axis=1), label=&#39;Testing error&#39;) plt.legend() plt.xlabel(&quot;Maximum depth of decision tree&quot;) plt.ylabel(&quot;Mean absolute error (k$)&quot;) _ = plt.title(&quot;Validation curve for decision tree&quot;) . Learning curve . Let&#39;s compute the learning curve for a decision tree and vary the proportion of the training set from 10% to 100%. . train_sizes = np.linspace(0.1, 1.0, num=5, endpoint=True) train_sizes . array([0.1 , 0.325, 0.55 , 0.775, 1. ]) . cv = ShuffleSplit(n_splits=30, test_size=0.2) . results = learning_curve( regressor, data, target, train_sizes=train_sizes, cv=cv, scoring=&quot;neg_mean_absolute_error&quot;, n_jobs=2) train_size, train_scores, test_scores = results[:3] # Convert the scores into errors train_errors, test_errors = -train_scores, -test_scores . plt.errorbar(train_size, train_errors.mean(axis=1), yerr=train_errors.std(axis=1), label=&quot;Training error&quot;) plt.errorbar(train_size, test_errors.mean(axis=1), yerr=test_errors.std(axis=1), label=&quot;Testing error&quot;) plt.legend() plt.xscale(&quot;log&quot;) plt.xlabel(&quot;Number of samples in the training set&quot;) plt.ylabel(&quot;Mean absolute error (k$)&quot;) _ = plt.title(&quot;Learning curve for decision tree&quot;) . Looking at the training error alone, we see that we get an error of 0 k$. It means that the trained model (i.e. decision tree) is clearly overfitting the training data. . Looking at the testing error alone, we observe that the more samples are added into the training set, the lower the testing error becomes. Also, we are searching for the plateau of the testing error for which there is no benefit to adding samples anymore or assessing the potential gain of adding more samples into the training set. . If we achieve a plateau and adding new samples in the training set does not reduce the testing error, we might have reach the Bayes error rate using the available model. Using a more complex model might be the only possibility to reduce the testing error further. .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/2021/07/22/OverAndUnderFitting.html",
            "relUrl": "/sklearn/2021/07/22/OverAndUnderFitting.html",
            "date": " ‚Ä¢ Jul 22, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Using numerical and categorical variables together",
            "content": "Preparation . import pandas as pd import matplotlib.pyplot as plt import time from sklearn.compose import make_column_selector as selector from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline from sklearn.model_selection import cross_validate from sklearn.preprocessing import OrdinalEncoder from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingClassifier . myDataFrame = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . myDataFrame = myDataFrame.drop(columns=&quot;education-num&quot;) . target_column = &#39;class&#39; target = myDataFrame[target_column] . pie = target.value_counts(normalize=True) pie . &lt;=50K 0.760718 &gt;50K 0.239282 Name: class, dtype: float64 . pie.plot(kind=&quot;pie&quot;, label=&quot;target&quot;); . data = myDataFrame.drop(columns=target_column) . print(f&quot;The dataset data contains {data.shape[0]} samples and {data.shape[1]} features&quot;) . The dataset data contains 48842 samples and 12 features . data.dtypes . age int64 workclass object education object marital-status object occupation object relationship object race object sex object capital-gain int64 capital-loss int64 hours-per-week int64 native-country object dtype: object . numerical_columns = selector(dtype_exclude=object)(data) . categorical_columns = selector(dtype_include=object)(data) . all_columns = numerical_columns + categorical_columns data = data[all_columns] . print(f&quot;The dataset data contains {data.shape[0]} samples and {data.shape[1]} features&quot;) . The dataset data contains 48842 samples and 12 features . data_numerical = data[numerical_columns] data_categorical = data[categorical_columns] . Numerical . Normalization + Regression . model = make_pipeline( StandardScaler(), LogisticRegression()) . cv_results = cross_validate(model, data_numerical, target, cv=10) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.800 +/- 0.004, for 0.077 seconds . Categorical . Hot encoding + Regression = good . model = make_pipeline( OneHotEncoder(handle_unknown=&quot;ignore&quot;), LogisticRegression(max_iter=500) ) . cv_results = cross_validate(model, data_categorical, target, cv=10) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.833 +/- 0.003, for 0.706 seconds . Ordinal encoding + Regression = not good . model = make_pipeline( OrdinalEncoder(handle_unknown=&quot;use_encoded_value&quot;, unknown_value=-1), LogisticRegression(max_iter=500) ) . cv_results = cross_validate(model, data_categorical, target, cv=10) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.755 +/- 0.002, for 0.354 seconds . Numerical &amp; Categorical . Normalize + Hot encoding + Linear Regression = good . Linear models are nice because they are usually cheap to train, small to deploy, fast to predict and give a good baseline. . categorical_preprocessor = OneHotEncoder(handle_unknown=&quot;ignore&quot;) numerical_preprocessor = StandardScaler() . preprocessor = ColumnTransformer([ (&#39;one-hot-encoder&#39;, categorical_preprocessor, categorical_columns), (&#39;standard-scaler&#39;, numerical_preprocessor, numerical_columns)]) . model = make_pipeline( preprocessor, LogisticRegression(max_iter=1500)) . cv_results = cross_validate(model, data, target, cv=10) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.851 +/- 0.003, for 1.062 seconds . Ordinal encoding + Gradient-boosting trees = best . For tree-based models, the handling of numerical and categorical variables is simpler than for linear models: . we do not need to scale the numerical features | using an ordinal encoding for the categorical variables is fine even if the encoding results in an arbitrary ordering | . categorical_preprocessor_tree = OrdinalEncoder( handle_unknown=&quot;use_encoded_value&quot;, unknown_value=-1) . preprocessor_tree = ColumnTransformer([ (&#39;categorical&#39;, categorical_preprocessor_tree, categorical_columns)], remainder=&quot;passthrough&quot;) . model = make_pipeline( preprocessor_tree, HistGradientBoostingClassifier()) . cv_results = cross_validate(model, data, target, cv=10, return_estimator=True) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.874 +/- 0.003, for 1.791 seconds .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/2021/06/08/NumericalCategoricalTogether.html",
            "relUrl": "/sklearn/2021/06/08/NumericalCategoricalTogether.html",
            "date": " ‚Ä¢ Jun 8, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Encoding of categorical variables",
            "content": "Preparation . import pandas as pd import time from sklearn.compose import make_column_selector as selector from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline from sklearn.model_selection import cross_validate from sklearn.preprocessing import OrdinalEncoder from sklearn.preprocessing import OneHotEncoder . myDataFrame = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . myDataFrame = myDataFrame.drop(columns=&quot;education-num&quot;) . The set . target_column = &#39;class&#39; target = myDataFrame[target_column] target.value_counts() . &lt;=50K 37155 &gt;50K 11687 Name: class, dtype: int64 . target.value_counts(normalize=True) . &lt;=50K 0.760718 &gt;50K 0.239282 Name: class, dtype: float64 . Continuation of preparation . data = myDataFrame.drop(columns=target_column) . print(f&quot;The dataset data contains {data.shape[0]} samples and {data.shape[1]} features&quot;) . The dataset data contains 48842 samples and 12 features . data.dtypes . age int64 workclass object education object marital-status object occupation object relationship object race object sex object capital-gain int64 capital-loss int64 hours-per-week int64 native-country object dtype: object . numerical_columns = selector(dtype_include=&quot;int64&quot;)(data) . categorical_columns = selector(dtype_include=&quot;object&quot;)(data) . all_columns = numerical_columns + categorical_columns data = data[all_columns] . print(f&quot;The dataset data contains {data.shape[0]} samples and {data.shape[1]} features&quot;) . The dataset data contains 48842 samples and 12 features . data_numerical = data[numerical_columns] data_categorical = data[categorical_columns] . Numerical . data_n_train, data_n_test, target_n_train, target_n_test = train_test_split( data_numerical, target, #random_state=42, test_size=0.25) . Cross validation + normalization . model = make_pipeline( StandardScaler(), LogisticRegression()) . cv_results = cross_validate(model, data_numerical, target, cv=10) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.800 +/- 0.004, for 0.074 seconds . Categorical . Encoding ordinal categories . data_categorical[:5] . workclass education marital-status occupation relationship race sex native-country . 0 Private | 11th | Never-married | Machine-op-inspct | Own-child | Black | Male | United-States | . 1 Private | HS-grad | Married-civ-spouse | Farming-fishing | Husband | White | Male | United-States | . 2 Local-gov | Assoc-acdm | Married-civ-spouse | Protective-serv | Husband | White | Male | United-States | . 3 Private | Some-college | Married-civ-spouse | Machine-op-inspct | Husband | Black | Male | United-States | . 4 ? | Some-college | Never-married | ? | Own-child | White | Female | United-States | . encoder_ordinal = OrdinalEncoder() . data_encoded_ordinal = encoder_ordinal.fit_transform(data_categorical) data_encoded_ordinal[:5] . array([[ 4., 1., 4., 7., 3., 2., 1., 39.], [ 4., 11., 2., 5., 0., 4., 1., 39.], [ 2., 7., 2., 11., 0., 4., 1., 39.], [ 4., 15., 2., 7., 0., 2., 1., 39.], [ 0., 15., 4., 0., 3., 4., 0., 39.]]) . encoder_ordinal.categories_ . [array([&#39; ?&#39;, &#39; Federal-gov&#39;, &#39; Local-gov&#39;, &#39; Never-worked&#39;, &#39; Private&#39;, &#39; Self-emp-inc&#39;, &#39; Self-emp-not-inc&#39;, &#39; State-gov&#39;, &#39; Without-pay&#39;], dtype=object), array([&#39; 10th&#39;, &#39; 11th&#39;, &#39; 12th&#39;, &#39; 1st-4th&#39;, &#39; 5th-6th&#39;, &#39; 7th-8th&#39;, &#39; 9th&#39;, &#39; Assoc-acdm&#39;, &#39; Assoc-voc&#39;, &#39; Bachelors&#39;, &#39; Doctorate&#39;, &#39; HS-grad&#39;, &#39; Masters&#39;, &#39; Preschool&#39;, &#39; Prof-school&#39;, &#39; Some-college&#39;], dtype=object), array([&#39; Divorced&#39;, &#39; Married-AF-spouse&#39;, &#39; Married-civ-spouse&#39;, &#39; Married-spouse-absent&#39;, &#39; Never-married&#39;, &#39; Separated&#39;, &#39; Widowed&#39;], dtype=object), array([&#39; ?&#39;, &#39; Adm-clerical&#39;, &#39; Armed-Forces&#39;, &#39; Craft-repair&#39;, &#39; Exec-managerial&#39;, &#39; Farming-fishing&#39;, &#39; Handlers-cleaners&#39;, &#39; Machine-op-inspct&#39;, &#39; Other-service&#39;, &#39; Priv-house-serv&#39;, &#39; Prof-specialty&#39;, &#39; Protective-serv&#39;, &#39; Sales&#39;, &#39; Tech-support&#39;, &#39; Transport-moving&#39;], dtype=object), array([&#39; Husband&#39;, &#39; Not-in-family&#39;, &#39; Other-relative&#39;, &#39; Own-child&#39;, &#39; Unmarried&#39;, &#39; Wife&#39;], dtype=object), array([&#39; Amer-Indian-Eskimo&#39;, &#39; Asian-Pac-Islander&#39;, &#39; Black&#39;, &#39; Other&#39;, &#39; White&#39;], dtype=object), array([&#39; Female&#39;, &#39; Male&#39;], dtype=object), array([&#39; ?&#39;, &#39; Cambodia&#39;, &#39; Canada&#39;, &#39; China&#39;, &#39; Columbia&#39;, &#39; Cuba&#39;, &#39; Dominican-Republic&#39;, &#39; Ecuador&#39;, &#39; El-Salvador&#39;, &#39; England&#39;, &#39; France&#39;, &#39; Germany&#39;, &#39; Greece&#39;, &#39; Guatemala&#39;, &#39; Haiti&#39;, &#39; Holand-Netherlands&#39;, &#39; Honduras&#39;, &#39; Hong&#39;, &#39; Hungary&#39;, &#39; India&#39;, &#39; Iran&#39;, &#39; Ireland&#39;, &#39; Italy&#39;, &#39; Jamaica&#39;, &#39; Japan&#39;, &#39; Laos&#39;, &#39; Mexico&#39;, &#39; Nicaragua&#39;, &#39; Outlying-US(Guam-USVI-etc)&#39;, &#39; Peru&#39;, &#39; Philippines&#39;, &#39; Poland&#39;, &#39; Portugal&#39;, &#39; Puerto-Rico&#39;, &#39; Scotland&#39;, &#39; South&#39;, &#39; Taiwan&#39;, &#39; Thailand&#39;, &#39; Trinadad&amp;Tobago&#39;, &#39; United-States&#39;, &#39; Vietnam&#39;, &#39; Yugoslavia&#39;], dtype=object)] . print(f&quot;The dataset original contains {data_categorical.shape[1]} features&quot;) . The dataset original contains 8 features . print(f&quot;The dataset encoded contains {data_encoded_ordinal.shape[1]} features&quot;) . The dataset encoded contains 8 features . Hot One encoding nominal categories (without assuming any order) . data_categorical[:2] . workclass education marital-status occupation relationship race sex native-country . 0 Private | 11th | Never-married | Machine-op-inspct | Own-child | Black | Male | United-States | . 1 Private | HS-grad | Married-civ-spouse | Farming-fishing | Husband | White | Male | United-States | . encoder_onehot = OneHotEncoder(sparse=False) . data_encoded_onehot = encoder_onehot.fit_transform(data_categorical) data_encoded_onehot[:2] . array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]) . print(f&quot;The dataset original contains {data_categorical.shape[1]} features&quot;) . The dataset original contains 8 features . print(f&quot;The dataset encoded contains {data_encoded_onehot.shape[1]} features&quot;) . The dataset encoded contains 102 features . Choosing an encoding strategy . Hot encoding + Regression = good . model_oneHotLin = make_pipeline( OneHotEncoder(handle_unknown=&quot;ignore&quot;), LogisticRegression(max_iter=1500) ) . cv_results = cross_validate(model_oneHotLin, data_categorical, target, cv=10) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.833 +/- 0.003, for 0.750 seconds . Ordinal encoding + Regression = not good . model_ordLin = make_pipeline( OrdinalEncoder(handle_unknown=&quot;use_encoded_value&quot;, unknown_value=100), LogisticRegression(max_iter=500) ) . cv_results = cross_validate(model_ordLin, data_categorical, target, cv=10) . scores = cv_results[&quot;test_score&quot;] fit_time = cv_results[&quot;fit_time&quot;] print(&quot;The accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds&quot;) . The accuracy is 0.755 +/- 0.002, for 0.363 seconds .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/2021/06/07/CategoricalVariables.html",
            "relUrl": "/sklearn/2021/06/07/CategoricalVariables.html",
            "date": " ‚Ä¢ Jun 7, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Combine sequential operations",
            "content": "Preparation . import pandas as pd import time from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline from sklearn.model_selection import cross_validate . from sklearn import set_config set_config(display=&#39;diagram&#39;) . myDataFrame = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/penguins_classification.csv&quot;) . The set . target_column = &#39;Species&#39; target = myDataFrame[target_column] target.value_counts() . Adelie 151 Gentoo 123 Chinstrap 68 Name: Species, dtype: int64 . target.value_counts(normalize=True) . Adelie 0.441520 Gentoo 0.359649 Chinstrap 0.198830 Name: Species, dtype: float64 . Continuation of preparation . data = myDataFrame.drop(columns=target_column) data.columns . Index([&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;], dtype=&#39;object&#39;) . numerical_columns = [&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;] data_numeric = data[numerical_columns] . data_train, data_test, target_train, target_test = train_test_split( data_numeric, target, #random_state=42, test_size=0.25) . data_train.describe() . Culmen Length (mm) Culmen Depth (mm) . count 256.000000 | 256.000000 | . mean 43.847656 | 17.101953 | . std 5.381675 | 1.913470 | . min 32.100000 | 13.200000 | . 25% 39.500000 | 15.675000 | . 50% 43.900000 | 17.300000 | . 75% 48.500000 | 18.600000 | . max 55.900000 | 21.500000 | . Model without normalization . model = LogisticRegression() start = time.time() model.fit(data_train, target_train) elapsed_time = time.time() - start . model_name = model.__class__.__name__ score = model.score(data_test, target_test) . print(f&quot;The accuracy using a {model_name} is {score:.3f} &quot; f&quot;with a fitting time of {elapsed_time:.3f} seconds &quot; f&quot;in {model.n_iter_[0]} iterations&quot;) . The accuracy using a LogisticRegression is 0.953 with a fitting time of 0.029 seconds in 62 iterations . Model with normalization : Pipeline . Fewer iterations . model = make_pipeline(StandardScaler(), LogisticRegression()) . start = time.time() model.fit(data_train, target_train) elapsed_time = time.time() - start . predicted_target = model.predict(data_test) model_name = model.__class__.__name__ score = model.score(data_test, target_test) . print(f&quot;The accuracy using a {model_name} is {score:.3f} &quot; f&quot;with a fitting time of {elapsed_time:.3f} seconds &quot; f&quot;in {model[-1].n_iter_[0]} iterations&quot;) . The accuracy using a Pipeline is 0.953 with a fitting time of 0.012 seconds in 14 iterations . Cross validation . model = make_pipeline(StandardScaler(), LogisticRegression()) . cv_result = cross_validate(model, data_numeric, target, cv=10) cv_result . {&#39;fit_time&#39;: array([0.00897551, 0.00997257, 0.00801015, 0.0069809 , 0.00698352, 0.00698447, 0.00598359, 0.00598407, 0.00599074, 0.00598288]), &#39;score_time&#39;: array([0.00299311, 0.00099707, 0.00099778, 0.00199199, 0.00099421, 0.00099444, 0.00099492, 0.00099397, 0.00098848, 0.00199533]), &#39;test_score&#39;: array([1. , 1. , 0.94117647, 0.97058824, 0.91176471, 0.94117647, 0.97058824, 0.97058824, 0.94117647, 0.94117647])} . scores = cv_result[&quot;test_score&quot;] fit_time = cv_result[&quot;fit_time&quot;] . print(&quot;The mean cross-validation accuracy is &quot; f&quot;{scores.mean():.3f} +/- {scores.std():.3f}, for {fit_time.mean():.3f} seconds in average&quot;) . The mean cross-validation accuracy is 0.959 +/- 0.027, for 0.008 seconds in average .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/2021/05/31/Pipeline.html",
            "relUrl": "/sklearn/2021/05/31/Pipeline.html",
            "date": " ‚Ä¢ May 31, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Normalization for numerical features",
            "content": "Preparation . import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler . myDataFrame = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/penguins_classification.csv&quot;) . The set . target_column = &#39;Species&#39; target = myDataFrame[target_column] target.value_counts() . Adelie 151 Gentoo 123 Chinstrap 68 Name: Species, dtype: int64 . target.value_counts(normalize=True) . Adelie 0.441520 Gentoo 0.359649 Chinstrap 0.198830 Name: Species, dtype: float64 . Continuation of preparation . data = myDataFrame.drop(columns=target_column) data.columns . Index([&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;], dtype=&#39;object&#39;) . numerical_columns = [&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;] data_numeric = data[numerical_columns] . data_train, data_test, target_train, target_test = train_test_split( data_numeric, target, #random_state=42, test_size=0.25) . data_train.describe() . Culmen Length (mm) Culmen Depth (mm) . count 256.000000 | 256.000000 | . mean 43.830469 | 17.151953 | . std 5.461854 | 1.917841 | . min 32.100000 | 13.100000 | . 25% 39.200000 | 15.675000 | . 50% 43.700000 | 17.300000 | . 75% 48.500000 | 18.600000 | . max 59.600000 | 21.500000 | . _ = data_train.hist(figsize=(10, 5)) . Normalization . scaler = StandardScaler() data_train_scaled = scaler.fit_transform(data_train) data_train_scaled = pd.DataFrame(data_train_scaled, columns=data_train.columns) data_train_scaled.describe() . Culmen Length (mm) Culmen Depth (mm) . count 2.560000e+02 | 2.560000e+02 | . mean -2.151057e-16 | 7.910339e-16 | . std 1.001959e+00 | 1.001959e+00 | . min -2.151915e+00 | -2.116907e+00 | . 25% -8.494440e-01 | -7.716210e-01 | . 50% -2.393405e-02 | 7.734577e-02 | . 75% 8.566099e-01 | 7.565192e-01 | . max 2.892868e+00 | 2.271598e+00 | . _ = data_train_scaled.hist(figsize=(10, 5)) . Conclusion . This transformer shifts and scales each feature individually so that they all have a 0-mean and a unit standard deviation. .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/2021/05/29/PreprocessingNumerical.html",
            "relUrl": "/sklearn/2021/05/29/PreprocessingNumerical.html",
            "date": " ‚Ä¢ May 29, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Dummy Classifier - Useful",
            "content": "The Dummy classifier will give us an idea of the &quot;minimum&quot; quality we can achieve. . It returns either a fixed value or the most frequent value of the training sample. . The quality of its score will be used as a floor for the future estimation. The objective is to do better or much better than the idiot! . Preparation . import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.dummy import DummyClassifier . myDataFrame = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/penguins_classification.csv&quot;) . The set . target_column = &#39;Species&#39; target = myDataFrame[target_column] target.value_counts() . Adelie 151 Gentoo 123 Chinstrap 68 Name: Species, dtype: int64 . Here we have the weight of each class, so also the minimum quality of the estimate . target.value_counts(normalize=True) . Adelie 0.441520 Gentoo 0.359649 Chinstrap 0.198830 Name: Species, dtype: float64 . Continuation of preparation . data = myDataFrame.drop(columns=target_column) data.columns . Index([&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;], dtype=&#39;object&#39;) . numerical_columns = [&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;] data_numeric = data[numerical_columns] . data_train, data_test, target_train, target_test = train_test_split( data_numeric, target, #random_state=42, test_size=0.25) . The dummy model . Prior (default) same as most frequent . The value returne is the most frequent in the training set . model = DummyClassifier(strategy=&#39;prior&#39;) . model = DummyClassifier() model.fit(data_train, target_train); . a = model.predict(data_test) n = a.size unique, counts = np.unique(a, return_counts=True) dict(zip(unique, counts/n)) . {&#39;Adelie&#39;: 1.0} . accuracy = model.score(data_test, target_test) print(f&quot;Accuracy of logistic regression: {accuracy:.3f}&quot;) . Accuracy of logistic regression: 0.465 . Stratified . The value return is found randomly by respecting the class distribution of the training . model = DummyClassifier(strategy=&#39;stratified&#39;, random_state= oneInt) . model = DummyClassifier(strategy=&#39;stratified&#39;) model.fit(data_train, target_train); . a = model.predict(data_test) n = a.size unique, counts = np.unique(a, return_counts=True) dict(zip(unique, counts/n)) . {&#39;Adelie&#39;: 0.5116279069767442, &#39;Chinstrap&#39;: 0.16279069767441862, &#39;Gentoo&#39;: 0.32558139534883723} . accuracy = model.score(data_test, target_test) print(f&quot;Accuracy of logistic regression: {accuracy:.3f}&quot;) . Accuracy of logistic regression: 0.337 . Uniform . The value return is generated uniformly at random . model = DummyClassifier(strategy=&#39;uniform&#39;, random_state= oneInt) . model = DummyClassifier(strategy=&#39;uniform&#39;) model.fit(data_train, target_train); . a = model.predict(data_test) n = a.size unique, counts = np.unique(a, return_counts=True) dict(zip(unique, counts/n)) . {&#39;Adelie&#39;: 0.3488372093023256, &#39;Chinstrap&#39;: 0.3023255813953488, &#39;Gentoo&#39;: 0.3488372093023256} . accuracy = model.score(data_test, target_test) print(f&quot;Accuracy of logistic regression: {accuracy:.3f}&quot;) . Accuracy of logistic regression: 0.372 . Constant . Always predicts a constant label that is provided. This is useful for metrics that evaluate a non-majority class . model = DummyClassifier(strategy=&#39;constant&#39;, constant=&quot;oneConstant&quot;) . model = DummyClassifier(strategy=&#39;constant&#39;, constant=&quot;Chinstrap&quot;) model.fit(data_train, target_train); . a = model.predict(data_test) n = a.size unique, counts = np.unique(a, return_counts=True) dict(zip(unique, counts/n)) . {&#39;Chinstrap&#39;: 1.0} . accuracy = model.score(data_test, target_test) print(f&quot;Accuracy of logistic regression: {accuracy:.3f}&quot;) . Accuracy of logistic regression: 0.221 . Conclusion . The best estimation is to prior, so any model better than that is good. . We thus have a floor value, then obviously the higher the score the better the estimation. .",
            "url": "https://cecilegallioz.github.io/blog/sklearn/2021/05/28/DummyClassifier.html",
            "relUrl": "/sklearn/2021/05/28/DummyClassifier.html",
            "date": " ‚Ä¢ May 28, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Basic model with scikit-learn",
            "content": "Imports . import pandas as pd . myDataFrame = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/adult-census.csv&quot;) . First analysis . print(f&quot;The dataset contains {myDataFrame.shape[0]} samples and &quot; f&quot;{myDataFrame.shape[1]} columns&quot;) . The dataset contains 48842 samples and 14 columns . myDataFrame.head() . age workclass education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country class . 0 25 | Private | 11th | 7 | Never-married | Machine-op-inspct | Own-child | Black | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 1 38 | Private | HS-grad | 9 | Married-civ-spouse | Farming-fishing | Husband | White | Male | 0 | 0 | 50 | United-States | &lt;=50K | . 2 28 | Local-gov | Assoc-acdm | 12 | Married-civ-spouse | Protective-serv | Husband | White | Male | 0 | 0 | 40 | United-States | &gt;50K | . 3 44 | Private | Some-college | 10 | Married-civ-spouse | Machine-op-inspct | Husband | Black | Male | 7688 | 0 | 40 | United-States | &gt;50K | . 4 18 | ? | Some-college | 10 | Never-married | ? | Own-child | White | Female | 0 | 0 | 30 | United-States | &lt;=50K | . Which column is our target to predict? . target_column = &#39;class&#39; target_y = myDataFrame[&quot;class&quot;] data_X = myDataFrame.drop(columns=&quot;class&quot;) . target_y.value_counts() . &lt;=50K 37155 &gt;50K 11687 Name: class, dtype: int64 . data_X.head() . age workclass education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country . 0 25 | Private | 11th | 7 | Never-married | Machine-op-inspct | Own-child | Black | Male | 0 | 0 | 40 | United-States | . 1 38 | Private | HS-grad | 9 | Married-civ-spouse | Farming-fishing | Husband | White | Male | 0 | 0 | 50 | United-States | . 2 28 | Local-gov | Assoc-acdm | 12 | Married-civ-spouse | Protective-serv | Husband | White | Male | 0 | 0 | 40 | United-States | . 3 44 | Private | Some-college | 10 | Married-civ-spouse | Machine-op-inspct | Husband | Black | Male | 7688 | 0 | 40 | United-States | . 4 18 | ? | Some-college | 10 | Never-married | ? | Own-child | White | Female | 0 | 0 | 30 | United-States | . Crosstab . Useful to detect columns containing the same information in two different forms (thus correlated). If this is the case, one of the columns is excluded. Here we excluded &quot;education-num&quot;. . pd.crosstab(index=data_X[&#39;education&#39;], columns=data_X[&#39;education-num&#39;]) . education-num 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 . education . 10th 0 | 0 | 0 | 0 | 0 | 1389 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 11th 0 | 0 | 0 | 0 | 0 | 0 | 1812 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12th 0 | 0 | 0 | 0 | 0 | 0 | 0 | 657 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1st-4th 0 | 247 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5th-6th 0 | 0 | 509 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7th-8th 0 | 0 | 0 | 955 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9th 0 | 0 | 0 | 0 | 756 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Assoc-acdm 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1601 | 0 | 0 | 0 | 0 | . Assoc-voc 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2061 | 0 | 0 | 0 | 0 | 0 | . Bachelors 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8025 | 0 | 0 | 0 | . Doctorate 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 594 | . HS-grad 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15784 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Masters 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2657 | 0 | 0 | . Preschool 83 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Prof-school 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 834 | 0 | . Some-college 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 10878 | 0 | 0 | 0 | 0 | 0 | 0 | . data_X = data_X.drop(columns=&quot;education-num&quot;) . Separation between numerical and categorical columns . print(f&quot;The dataset data_X contains {data_X.shape[0]} samples and &quot; f&quot;{data_X.shape[1]} columns&quot;) . The dataset data_X contains 48842 samples and 12 columns . data_X.dtypes . age int64 workclass object education object marital-status object occupation object relationship object race object sex object capital-gain int64 capital-loss int64 hours-per-week int64 native-country object dtype: object . We sort the variable names according to their type . by hand . numerical_columns = [&quot;age&quot;, &quot;capital-gain&quot;, &quot;capital-loss&quot;, &quot;hours-per-week&quot;] categorical_columns = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;, &#39;sex&#39;, &#39;native-country&#39;] . select columns based on their data type . from sklearn.compose import make_column_selector as selector . categorical_columns = selector(dtype_include=&quot;object&quot;)(data_X) categorical_columns . [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;, &#39;sex&#39;, &#39;native-country&#39;] . numerical_columns = selector(dtype_include=&quot;int64&quot;)(data_X) numerical_columns . [&#39;age&#39;, &#39;capital-gain&#39;, &#39;capital-loss&#39;, &#39;hours-per-week&#39;] . all_columns = numerical_columns + categorical_columns data_X = data_X[all_columns] . print(f&quot;The dataset data_X contains {data_X.shape[0]} samples and &quot; f&quot;{data_X.shape[1]} columns&quot;) . The dataset data_X contains 48842 samples and 12 columns . data_X[numerical_columns].describe() . age capital-gain capital-loss hours-per-week . count 48842.000000 | 48842.000000 | 48842.000000 | 48842.000000 | . mean 38.643585 | 1079.067626 | 87.502314 | 40.422382 | . std 13.710510 | 7452.019058 | 403.004552 | 12.391444 | . min 17.000000 | 0.000000 | 0.000000 | 1.000000 | . 25% 28.000000 | 0.000000 | 0.000000 | 40.000000 | . 50% 37.000000 | 0.000000 | 0.000000 | 40.000000 | . 75% 48.000000 | 0.000000 | 0.000000 | 45.000000 | . max 90.000000 | 99999.000000 | 4356.000000 | 99.000000 | . data_X_numerical = data_X[numerical_columns] . The model . Train-test split the dataset . from sklearn.model_selection import train_test_split data_train, data_test, target_train, target_test = train_test_split( data_X_numerical, target_y, random_state=42, test_size=0.25) . print(f&quot;Number of samples in testing: {data_test.shape[0]} =&gt; &quot; f&quot;{data_test.shape[0] / data_X_numerical.shape[0] * 100:.1f}% of the&quot; f&quot; original set&quot;) . Number of samples in testing: 12211 =&gt; 25.0% of the original set . print(f&quot;Number of samples in training: {data_train.shape[0]} =&gt; &quot; f&quot;{data_train.shape[0] / data_X_numerical.shape[0] * 100:.1f}% of the&quot; f&quot; original set&quot;) . Number of samples in training: 36631 =&gt; 75.0% of the original set . To display nice model diagram . from sklearn import set_config set_config(display=&#39;diagram&#39;) . To create a logistic regression model in scikit-learn . from sklearn.linear_model import LogisticRegression model = LogisticRegression() . Use the fit method to train the model using the training data and labels . model.fit(data_train, target_train) . &lt;div id=&quot;sk-4c18ffa1-4a6f-4894-bde5-f4ed278b4c41&quot; class&quot;sk-top-container&quot;&gt;LogisticRegressionLogisticRegression() . Use the score method to check the model statistical performance on the test set . accuracy = model.score(data_test, target_test) print(f&quot;Accuracy of logistic regression: {accuracy:.3f}&quot;) . Accuracy of logistic regression: 0.807 . &lt;/div&gt; .",
            "url": "https://cecilegallioz.github.io/blog/panda/sklearn/2021/05/21/FirstRegression.html",
            "relUrl": "/panda/sklearn/2021/05/21/FirstRegression.html",
            "date": " ‚Ä¢ May 21, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Data set analyse",
            "content": "Auto-scrolling . To disable auto-scrolling, execute this javascript in a notebook cell before other cells are executed &#39;source stackoverflow&#39; . %%javascript IPython.OutputArea.prototype._should_scroll = function(lines) { return false; } . Imports . import pandas as pd import seaborn as sns . myDataFrame = pd.read_csv(&quot;../../scikit-learn-mooc/datasets/penguins_classification.csv&quot;) . First analysis . print(f&quot;The dataset contains {myDataFrame.shape[0]} samples and &quot; f&quot;{myDataFrame.shape[1]} columns&quot;) . The dataset contains 342 samples and 3 columns . myDataFrame.columns . Index([&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;, &#39;Species&#39;], dtype=&#39;object&#39;) . myDataFrame.head() . Culmen Length (mm) Culmen Depth (mm) Species . 0 39.1 | 18.7 | Adelie | . 1 39.5 | 17.4 | Adelie | . 2 40.3 | 18.0 | Adelie | . 3 36.7 | 19.3 | Adelie | . 4 39.3 | 20.6 | Adelie | . Which column is our target to predict? . target_column = &#39;Species&#39; . myDataFrame[target_column].value_counts() . Adelie 151 Gentoo 123 Chinstrap 68 Name: Species, dtype: int64 . Separation between numerical and categorical columns . Type of the objects . myDataFrame.dtypes . Culmen Length (mm) float64 Culmen Depth (mm) float64 Species object dtype: object . We sort the variable names according to their type . numerical_columns = [&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;] categorical_columns = [] all_columns = numerical_columns + categorical_columns + [target_column] myDataFrame = myDataFrame[all_columns] myDataFrame.columns . Index([&#39;Culmen Length (mm)&#39;, &#39;Culmen Depth (mm)&#39;, &#39;Species&#39;], dtype=&#39;object&#39;) . To look at the amplitude and distribution of the data . . Note: the &quot;_&quot; is to store a variable that we will not reuse . myDataFrame[numerical_columns].describe() . Culmen Length (mm) Culmen Depth (mm) . count 342.000000 | 342.000000 | . mean 43.921930 | 17.151170 | . std 5.459584 | 1.974793 | . min 32.100000 | 13.100000 | . 25% 39.225000 | 15.600000 | . 50% 44.450000 | 17.300000 | . 75% 48.500000 | 18.700000 | . max 59.600000 | 21.500000 | . _ = myDataFrame.hist(figsize=(10, 5)) . Same with seaborn . seaborn.pairplot . _ = sns.pairplot(myDataFrame) . To detect link between the features and the target column . _ = sns.pairplot(myDataFrame, height=4, hue=target_column, corner=True) . Idem but with circle of &quot;same&quot; data . g = sns.pairplot(myDataFrame, height=4, hue=target_column, corner=True) g.map_lower(sns.kdeplot, levels=3, color=&quot;.2&quot;); . Crosstab . Useful to detect columns containing the same information in two different forms (thus correlated). If this is the case, one of the columns is excluded. . Here, we don&#39;t see this kind of link . pd.crosstab(index=myDataFrame[numerical_columns[0]], columns=myDataFrame[numerical_columns[1]]) . Culmen Depth (mm) 13.1 13.2 13.3 13.4 13.5 13.6 13.7 13.8 13.9 14.0 ... 20.1 20.2 20.3 20.5 20.6 20.7 20.8 21.1 21.2 21.5 . Culmen Length (mm) . 32.1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 33.1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 33.5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 34.0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 34.1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 55.1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 55.8 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 55.9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 58.0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 59.6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 164 rows √ó 80 columns .",
            "url": "https://cecilegallioz.github.io/blog/panda/seaborn/2021/05/20/SKLv1M1C1Exploration.html",
            "relUrl": "/panda/seaborn/2021/05/20/SKLv1M1C1Exploration.html",
            "date": " ‚Ä¢ May 20, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "A little on PyTorch",
            "content": "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. . She is much faster than NumPy with similar functionality. NumPy array and PyTorch tensor are both multidimensional table of data, with all items of the same type. . import torch . data = [[1,2,3],[4,5,6]] tns = torch.tensor(data) tns . tensor([[1, 2, 3], [4, 5, 6]]) . tns[1] . tensor([4, 5, 6]) . tns[:,1] . tensor([2, 5]) . tns[1,1:3] . tensor([5, 6]) . tns+1 . tensor([[2, 3, 4], [5, 6, 7]]) . tns.type() . &#39;torch.LongTensor&#39; . tns*1.5 . tensor([[1.5000, 3.0000, 4.5000], [6.0000, 7.5000, 9.0000]]) . torch.tensor([1,2,3]) + torch.tensor([1,1,1]) . tensor([2, 3, 4]) . torch.tensor([0]) + torch.tensor([1,1,1]) . tensor([1, 1, 1]) .",
            "url": "https://cecilegallioz.github.io/blog/pytorch/2021/04/30/ALittleofPyTorche.html",
            "relUrl": "/pytorch/2021/04/30/ALittleofPyTorche.html",
            "date": " ‚Ä¢ Apr 30, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Integrate LaTeX in Jupyter Notebook",
            "content": "Why ? . I wanted to try LaTeX because seeing a nice mathematical formula from time to time is nice. . Moreover, I realized that all my mathematics books during my studies were written with this tool, the font is very recognizable. So now nostalgia has caught up with me! And the usual desire to discover a new tool. . Install on my computer . On my computer, I use Visual Studio Code and the two extensions LaTeX Utilities and LaTeX Workshop and the LaTeX distribution TeX Live. Thanks to Linux Magazine n¬∞242 for the help. . Use in a Notebook . But here, I wanted to test LaTeX in a Jupyter NoteBook. . Inside of your Markdown cell: formula between two $ . integrated formula: $f(x) = 2 times x$ . $f(x) = 2 times x$ . | . . center formula: $$f(x) = 2 times x$$ . $$f(x) = 2 times x$$ . | . Cheat sheet on LaTeX . One page with a lot of informations A quick guide to LATEX, very useful. . Insert text inside a formula text . $$f(x) = 2x text{ with } 0 le x leq 5$$ . $$f(x) = 2x text{ with } 0 le x leq 5$$ . Vector and other hats . $$ overrightarrow{AB }$$ . $$ overrightarrow{AB }$$ . $$ overline{AB }$$ . $$ overline{AB }$$ . $$ widehat{ABC }$$ . $$ widehat{ABC }$$ . $$ vec{A }$$ . $$ vec{A }$$ . $$ vec{ imath}$$ . $$ vec{ imath}$$ . Predefined functions cos . $$ cos{(x+1)}$$ . $$ cos{(x+1)}$$ . $$ sqrt{(x+1)}$$ . $$ sqrt{(x+1)}$$ . $$ sqrt[3]{(x+1)}$$ . $$ sqrt[3]{(x+1)}$$ . Exponent and subscript . $$x_i+1=2^x-1$$ [hard to read but works] . $$x_i+1=2^x-1$$ . $$x_i + 1 = 2^x - 1$$ [same result] . $$x_i + 1 = 2^x - 1$$ . $$x_{i + 1} = 2^{x - 1}$$ . $$x_{i + 1} = 2^{x - 1}$$ . Limit, sum and integral . $$ lim_{x to infty} x + 1$$ . $$ lim_{x to infty} x + 1$$ . $$ sum_{i=1}^{10} x_i$$ . $$ sum_{i=1}^{10} x_i$$ . $$ prod_{i=1}^{10} sqrt[i]{x_i^2 - 1}$$ . $$ prod_{i=1}^{10} sqrt[i]{x_i^2 - 1}$$ . $$ int_{0}^{1} 2x ,dx$$ . $$ int_{0}^{1} 2x ,dx$$ . Parenthesis, bracket and brace . $$ cos{ left( frac{1}{x} right)}$$ . $$ cos{ left( frac{1}{x} right)}$$ . $$ left { begin{array}{ll} x-1=2 x+1=4 end{array} right.$$ . $$ left { begin{array}{ll} x-1=2 x+1=4 end{array} right.$$ . $$ overbrace{AB^2 + AC^2 = BC^2}^{ text{Pythagor&#39;s theorem}}$$ . $$ overbrace{AB^2 + AC^2 = BC^2}^{ text{Pythagor&#39;s theorem}}$$ . Matrix . $$ begin{pmatrix} a &amp; b &amp; c d &amp; e &amp; f g &amp; h &amp; i end{pmatrix}$$ . $$ begin{pmatrix} a &amp; b &amp; c d &amp; e &amp; f g &amp; h &amp; i end{pmatrix}$$ $$ begin{Bmatrix} a &amp; b &amp; c vdots &amp; vdots &amp; vdots g &amp; h &amp; i end{Bmatrix}$$ . $$ begin{Bmatrix} a &amp; b &amp; c vdots &amp; vdots &amp; vdots g &amp; h &amp; i end{Bmatrix}$$",
            "url": "https://cecilegallioz.github.io/blog/jupyter/latex/2021/04/27/IntegratedLatex.html",
            "relUrl": "/jupyter/latex/2021/04/27/IntegratedLatex.html",
            "date": " ‚Ä¢ Apr 27, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Part 1 on fastai book done!",
            "content": "Okay, what have I learned so far: . write a template to sort images | save it | fail to publish it with a lot of tools (work for later) | use DeepL to improve my written English | say no to unethical requests | be careful with over fitting | use fastai in the Titanic competition on Kaggle (top 16% - I‚Äôm proud!) | use Github and fastpages to create this blog | create an account on google analytics to reference this blog | create an account on Azure to use Bing on several images | . As for deploying a functional model on a web page, I have a few ideas: . Google Cloud | write a site on Django and integrate my template myself | look again at the Fastai courses to see if I can manage to use their solutions. | . That‚Äôs all for now! .",
            "url": "https://cecilegallioz.github.io/blog/fastai/2021/04/22/Goforpart2.html",
            "relUrl": "/fastai/2021/04/22/Goforpart2.html",
            "date": " ‚Ä¢ Apr 22, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "Image classifier model",
            "content": "Here, we are to use a model trained elsewhere (on Google Cloud). . Importing the pre-trained model . path = Path(&#39;./src&#39;) path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;src/exportBear.pkl&#39;)] . Note :use the one cell below if you are on Windows . import pathlib temp = pathlib.PosixPath pathlib.PosixPath = pathlib.WindowsPath . learn_inf = load_learner(path/&#39;exportBear.pkl&#39;) . learn_inf.dls.vocab . [&#39;black&#39;, &#39;grizzly&#39;, &#39;teddy&#39;] . Initialization . btn_upload = SimpleNamespace(data = [&#39;src/grizzly.jpg&#39;]) . img = PILImage.create(btn_upload.data[-1]) . lbl_pred = widgets.Label() lbl_pred.value = &quot;Your prediction here&quot; . Create the button to add an image to classify . btn_upload = widgets.FileUpload() . Create the button &quot;Classify&quot; to run the classification . btn_run = widgets.Button(description=&#39;Classify&#39;) . Display the added image . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) . Create the function that performs the classification from the model on the image . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . Create the widget containing all the elements . VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Note :use the one cell below if you are on Windows . pathlib.PosixPath = temp . . Note: To test my model in real life, I invite you to visit my github and launch this notebook on your own environment. . Picture of Initialization . Picture of classification of a photo .",
            "url": "https://cecilegallioz.github.io/blog/fastai/2021/04/19/ModelClassifierPicture.html",
            "relUrl": "/fastai/2021/04/19/ModelClassifierPicture.html",
            "date": " ‚Ä¢ Apr 19, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Insert HTML in a NoteBook",
            "content": "The fastpages blog engine is very useful to write a notebook. But I wanted to be able to write HTML directly in my notebook to implement a template. . For the moment, I&#39;m not going to do like that, I&#39;ll keep my template in a classic notebook, but I&#39;ll leave you here the technique to write HTML from a notebook which will be interpreted by fastpages in HTML. . This is HTML written in the notebook in a plain text cell (not Markdown), with h3 tags . If I try to write html with print in a code cell, it fails. . age = 45 name = &quot;Paul&quot; gap = 10 print(f&quot;&lt;h3&gt;In {gap} year(s), {name} will be {age + gap} year(s) old.&lt;/h3&gt;&quot;) . With this formula, a new plain text cell is created, so that&#39;s good! . get_ipython().run_cell_magic(u&#39;HTML&#39;, u&#39;&#39;,f&quot;&lt;h3&gt;In {gap} year(s), {name} will be {age + gap} year(s) old.&lt;/h3&gt;&quot;) . In 10 year(s), Paul will be 55 year(s) old. .",
            "url": "https://cecilegallioz.github.io/blog/html/fastpages/2021/04/18/TestHTML.html",
            "relUrl": "/html/fastpages/2021/04/18/TestHTML.html",
            "date": " ‚Ä¢ Apr 18, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Reminder on Panda (from Kaggle)",
            "content": "import pandas as pd pd.set_option(&quot;display.max_rows&quot;, 6) . DataFrame . pd.DataFrame({Column1: [Value1.1, Value2.1], Column2: [Value1.2, Value2.2]}, index=[&quot;Row1&quot;, &quot;Row2&quot;]) . DataFrame = table as Excel . pd.DataFrame({&quot;Column_1&quot;: [&quot;Value_1.1&quot;, &quot;Value_2.1&quot;], &quot;Column_2&quot;: [&quot;Value_1.2&quot;, &quot;Value_2.2&quot;]}, index=[&quot;Row_1&quot;, &quot;Row_2&quot;]) . Column_1 Column_2 . Row_1 Value_1.1 | Value_1.2 | . Row_2 Value_2.1 | Value_2.2 | . pd.DataFrame({&quot;Apples&quot;: [35, 41], &quot;Bananas&quot;: [21, 34]}, index=[&quot;2017 Sales&quot;, &quot;2018 Sales&quot;]) . Apples Bananas . 2017 Sales 35 | 21 | . 2018 Sales 41 | 34 | . Series . pd.Series([Value1, Value2, Value3], index=[&quot;Row1&quot;, &quot;Row2&quot;, &quot;Row3&quot;], name=&quot;nameSerie&quot;) . Series = list . pd.Series([&quot;Value_1&quot;, &quot;Value_2&quot;, &quot;Value_3&quot;], index=[&quot;Row1&quot;, &quot;Row2&quot;, &quot;Row3&quot;], name=&quot;nameSerie&quot;) . Row1 Value_1 Row2 Value_2 Row3 Value_3 Name: nameSerie, dtype: object . pd.Series([&quot;4 cups&quot;, &quot;1 cup&quot;, &quot;2 large&quot;, &quot;1 can&quot;], index=[&quot;Flour&quot;, &quot;Milk&quot;, &quot;Eggs&quot;, &quot;Spam&quot;], name=&quot;Dinner&quot;) . Flour 4 cups Milk 1 cup Eggs 2 large Spam 1 can Name: Dinner, dtype: object . Reading . pd.read_csv(&quot;path&quot;, index_col=0) . Put the option &quot;index_col=0&quot; if you want to use the index inside the csv (in first column) . wines = pd.read_csv(&quot;./src/winemag-data-130k-v2.csv&quot;, index_col=0) . wines.shape . (129971, 13) . wines.head() . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 0 Italy | Aromas include tropical fruit, broom, brimston... | Vulk√† Bianco | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Nicosia 2013 Vulk√† Bianco (Etna) | White Blend | Nicosia | . 1 Portugal | This is ripe and fruity, a wine that is smooth... | Avidagos | 87 | 15.0 | Douro | NaN | NaN | Roger Voss | @vossroger | Quinta dos Avidagos 2011 Avidagos Red (Douro) | Portuguese Red | Quinta dos Avidagos | . 2 US | Tart and snappy, the flavors of lime flesh and... | NaN | 87 | 14.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Rainstorm 2013 Pinot Gris (Willamette Valley) | Pinot Gris | Rainstorm | . 3 US | Pineapple rind, lemon pith and orange blossom ... | Reserve Late Harvest | 87 | 13.0 | Michigan | Lake Michigan Shore | NaN | Alexander Peartree | NaN | St. Julian 2013 Reserve Late Harvest Riesling ... | Riesling | St. Julian | . 4 US | Much like the regular bottling from 2012, this... | Vintner&#39;s Reserve Wild Child Block | 87 | 65.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Sweet Cheeks 2012 Vintner&#39;s Reserve Wild Child... | Pinot Noir | Sweet Cheeks | . . Index with iloc (numerical position) . data.iloc[row, column] . &quot;:&quot; minds everithing . iloc uses the Python stdlib indexing scheme, where the first element of the range is included and the last one excluded. . Warning: with iloc : [0:10] =&gt; 0,...,9 . wines.iloc[0] . country Italy description Aromas include tropical fruit, broom, brimston... designation Vulk√† Bianco ... title Nicosia 2013 Vulk√† Bianco (Etna) variety White Blend winery Nicosia Name: 0, Length: 13, dtype: object . wines.iloc[:, 0] . 0 Italy 1 Portugal 2 US ... 129968 France 129969 France 129970 France Name: country, Length: 129971, dtype: object . wines.iloc[:3, 0] . 0 Italy 1 Portugal 2 US Name: country, dtype: object . wines.iloc[1:3, 0] . 1 Portugal 2 US Name: country, dtype: object . wines.iloc[[0, 1, 2], 0] . 0 Italy 1 Portugal 2 US Name: country, dtype: object . wines.iloc[-5:] . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 129966 Germany | Notes of honeysuckle and cantaloupe sweeten th... | Brauneberger Juffer-Sonnenuhr Sp√§tlese | 90 | 28.0 | Mosel | NaN | NaN | Anna Lee C. Iijima | NaN | Dr. H. Thanisch (Erben M√ºller-Burggraef) 2013 ... | Riesling | Dr. H. Thanisch (Erben M√ºller-Burggraef) | . 129967 US | Citation is given as much as a decade of bottl... | NaN | 90 | 75.0 | Oregon | Oregon | Oregon Other | Paul Gregutt | @paulgwine | Citation 2004 Pinot Noir (Oregon) | Pinot Noir | Citation | . 129968 France | Well-drained gravel soil gives this wine its c... | Kritt | 90 | 30.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Gresser 2013 Kritt Gewurztraminer (Als... | Gew√ºrztraminer | Domaine Gresser | . 129969 France | A dry style of Pinot Gris, this is crisp with ... | NaN | 90 | 32.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Marcel Deiss 2012 Pinot Gris (Alsace) | Pinot Gris | Domaine Marcel Deiss | . 129970 France | Big, rich and off-dry, this is powered by inte... | Lieu-dit Harth Cuv√©e Caroline | 90 | 21.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Schoffit 2012 Lieu-dit Harth Cuv√©e Car... | Gew√ºrztraminer | Domaine Schoffit | . Index with loc (label_based position) . data.loc[row, column] . loc indexes inclusively. So 0:10 will select entries 0,...,10. . Warning: with loc : [0:10] =&gt; 0,...,10 . wines.loc[:, &quot;points&quot;] . 0 87 1 87 2 87 .. 129968 90 129969 90 129970 90 Name: points, Length: 129971, dtype: int64 . wines.loc[0:9,[&quot;country&quot;, &quot;variety&quot;]] . country variety . 0 Italy | White Blend | . 1 Portugal | Portuguese Red | . 2 US | Pinot Gris | . ... ... | ... | . 7 France | Gew√ºrztraminer | . 8 Germany | Gew√ºrztraminer | . 9 France | Pinot Gris | . 10 rows √ó 2 columns . wines.loc[(wines.country == &#39;Italy&#39;) &amp; (wines.points &gt;= 90)] . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 120 Italy | Slightly backward, particularly given the vint... | Bricco Rocche Prap√≥ | 92 | 70.0 | Piedmont | Barolo | NaN | NaN | NaN | Ceretto 2003 Bricco Rocche Prap√≥ (Barolo) | Nebbiolo | Ceretto | . 130 Italy | At the first it was quite muted and subdued, b... | Bricco Rocche Brunate | 91 | 70.0 | Piedmont | Barolo | NaN | NaN | NaN | Ceretto 2003 Bricco Rocche Brunate (Barolo) | Nebbiolo | Ceretto | . 133 Italy | Einaudi&#39;s wines have been improving lately, an... | NaN | 91 | 68.0 | Piedmont | Barolo | NaN | NaN | NaN | Poderi Luigi Einaudi 2003 Barolo | Nebbiolo | Poderi Luigi Einaudi | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 129947 Italy | A blend of 65% Cabernet Sauvignon, 30% Merlot ... | Symposio | 90 | 20.0 | Sicily &amp; Sardinia | Terre Siciliane | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Feudo Principi di Butera 2012 Symposio Red (Te... | Red Blend | Feudo Principi di Butera | . 129961 Italy | Intense aromas of wild cherry, baking spice, t... | NaN | 90 | 30.0 | Sicily &amp; Sardinia | Sicilia | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | COS 2013 Frappato (Sicilia) | Frappato | COS | . 129962 Italy | Blackberry, cassis, grilled herb and toasted a... | S√†gana Tenuta San Giacomo | 90 | 40.0 | Sicily &amp; Sardinia | Sicilia | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Cusumano 2012 S√†gana Tenuta San Giacomo Nero d... | Nero d&#39;Avola | Cusumano | . 6648 rows √ó 13 columns . wines.loc[(wines.country == &#39;Italy&#39;) | (wines.points &gt;= 90)] . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 0 Italy | Aromas include tropical fruit, broom, brimston... | Vulk√† Bianco | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Nicosia 2013 Vulk√† Bianco (Etna) | White Blend | Nicosia | . 6 Italy | Here&#39;s a bright, informal red that opens with ... | Belsito | 87 | 16.0 | Sicily &amp; Sardinia | Vittoria | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Terre di Giurfo 2013 Belsito Frappato (Vittoria) | Frappato | Terre di Giurfo | . 13 Italy | This is dominated by oak and oak-driven aromas... | Rosso | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Masseria Setteporte 2012 Rosso (Etna) | Nerello Mascalese | Masseria Setteporte | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 129968 France | Well-drained gravel soil gives this wine its c... | Kritt | 90 | 30.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Gresser 2013 Kritt Gewurztraminer (Als... | Gew√ºrztraminer | Domaine Gresser | . 129969 France | A dry style of Pinot Gris, this is crisp with ... | NaN | 90 | 32.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Marcel Deiss 2012 Pinot Gris (Alsace) | Pinot Gris | Domaine Marcel Deiss | . 129970 France | Big, rich and off-dry, this is powered by inte... | Lieu-dit Harth Cuv√©e Caroline | 90 | 21.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Schoffit 2012 Lieu-dit Harth Cuv√©e Car... | Gew√ºrztraminer | Domaine Schoffit | . 61937 rows √ó 13 columns . wines.loc[wines.country.isin([&#39;Italy&#39;, &#39;France&#39;])] . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 0 Italy | Aromas include tropical fruit, broom, brimston... | Vulk√† Bianco | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Nicosia 2013 Vulk√† Bianco (Etna) | White Blend | Nicosia | . 6 Italy | Here&#39;s a bright, informal red that opens with ... | Belsito | 87 | 16.0 | Sicily &amp; Sardinia | Vittoria | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Terre di Giurfo 2013 Belsito Frappato (Vittoria) | Frappato | Terre di Giurfo | . 7 France | This dry and restrained wine offers spice in p... | NaN | 87 | 24.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Trimbach 2012 Gewurztraminer (Alsace) | Gew√ºrztraminer | Trimbach | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 129968 France | Well-drained gravel soil gives this wine its c... | Kritt | 90 | 30.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Gresser 2013 Kritt Gewurztraminer (Als... | Gew√ºrztraminer | Domaine Gresser | . 129969 France | A dry style of Pinot Gris, this is crisp with ... | NaN | 90 | 32.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Marcel Deiss 2012 Pinot Gris (Alsace) | Pinot Gris | Domaine Marcel Deiss | . 129970 France | Big, rich and off-dry, this is powered by inte... | Lieu-dit Harth Cuv√©e Caroline | 90 | 21.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Schoffit 2012 Lieu-dit Harth Cuv√©e Car... | Gew√ºrztraminer | Domaine Schoffit | . 41633 rows √ó 13 columns . wines.loc[wines.price.notnull()] . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 1 Portugal | This is ripe and fruity, a wine that is smooth... | Avidagos | 87 | 15.0 | Douro | NaN | NaN | Roger Voss | @vossroger | Quinta dos Avidagos 2011 Avidagos Red (Douro) | Portuguese Red | Quinta dos Avidagos | . 2 US | Tart and snappy, the flavors of lime flesh and... | NaN | 87 | 14.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Rainstorm 2013 Pinot Gris (Willamette Valley) | Pinot Gris | Rainstorm | . 3 US | Pineapple rind, lemon pith and orange blossom ... | Reserve Late Harvest | 87 | 13.0 | Michigan | Lake Michigan Shore | NaN | Alexander Peartree | NaN | St. Julian 2013 Reserve Late Harvest Riesling ... | Riesling | St. Julian | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 129968 France | Well-drained gravel soil gives this wine its c... | Kritt | 90 | 30.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Gresser 2013 Kritt Gewurztraminer (Als... | Gew√ºrztraminer | Domaine Gresser | . 129969 France | A dry style of Pinot Gris, this is crisp with ... | NaN | 90 | 32.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Marcel Deiss 2012 Pinot Gris (Alsace) | Pinot Gris | Domaine Marcel Deiss | . 129970 France | Big, rich and off-dry, this is powered by inte... | Lieu-dit Harth Cuv√©e Caroline | 90 | 21.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Schoffit 2012 Lieu-dit Harth Cuv√©e Car... | Gew√ºrztraminer | Domaine Schoffit | . 120975 rows √ó 13 columns . wines.loc[wines.price.isnull()] . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 0 Italy | Aromas include tropical fruit, broom, brimston... | Vulk√† Bianco | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Nicosia 2013 Vulk√† Bianco (Etna) | White Blend | Nicosia | . 13 Italy | This is dominated by oak and oak-driven aromas... | Rosso | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Masseria Setteporte 2012 Rosso (Etna) | Nerello Mascalese | Masseria Setteporte | . 30 France | Red cherry fruit comes laced with light tannin... | Nouveau | 86 | NaN | Beaujolais | Beaujolais-Villages | NaN | Roger Voss | @vossroger | Domaine de la Madone 2012 Nouveau (Beaujolais... | Gamay | Domaine de la Madone | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 129863 Portugal | This mature wine that has 50% Touriga Nacional... | Reserva | 90 | NaN | D√£o | NaN | NaN | Roger Voss | @vossroger | Seacampo 2011 Reserva Red (D√£o) | Portuguese Red | Seacampo | . 129893 Italy | Aromas of passion fruit, hay and a vegetal not... | Corte Menini | 91 | NaN | Veneto | Soave Classico | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Le Mandolare 2015 Corte Menini (Soave Classico) | Garganega | Le Mandolare | . 129964 France | Initially quite muted, this wine slowly develo... | Domaine Saint-R√©my Herrenweg | 90 | NaN | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Ehrhart 2013 Domaine Saint-R√©my Herren... | Gew√ºrztraminer | Domaine Ehrhart | . 8996 rows √ó 13 columns . Summary functions . wines.points.describe() . count 129971.000000 mean 88.447138 std 3.039730 ... 50% 88.000000 75% 91.000000 max 100.000000 Name: points, Length: 8, dtype: float64 . wines.taster_name.describe() . count 103727 unique 19 top Roger Voss freq 25514 Name: taster_name, dtype: object . wines.points.mean() . 88.44713820775404 . wines.points.median() . 88.0 . wines.taster_name.unique() . array([&#39;Kerin O‚ÄôKeefe&#39;, &#39;Roger Voss&#39;, &#39;Paul Gregutt&#39;, &#39;Alexander Peartree&#39;, &#39;Michael Schachner&#39;, &#39;Anna Lee C. Iijima&#39;, &#39;Virginie Boone&#39;, &#39;Matt Kettmann&#39;, nan, &#39;Sean P. Sullivan&#39;, &#39;Jim Gordon&#39;, &#39;Joe Czerwinski&#39;, &#39;Anne Krebiehl xa0MW&#39;, &#39;Lauren Buzzeo&#39;, &#39;Mike DeSimone&#39;, &#39;Jeff Jenssen&#39;, &#39;Susan Kostrzewa&#39;, &#39;Carrie Dykes&#39;, &#39;Fiona Adams&#39;, &#39;Christina Pickard&#39;], dtype=object) . wines.taster_name.value_counts() . Roger Voss 25514 Michael Schachner 15134 Kerin O‚ÄôKeefe 10776 ... Carrie Dykes 139 Fiona Adams 27 Christina Pickard 6 Name: taster_name, Length: 19, dtype: int64 . Maps . mySeries.map(lambda p: function(p)) . . myDataFrame.apply(nameFunctionOnRow, axis=&quot;columns&quot;) myDataFrame.apply(nameFunctionOnColumns, axis=&quot;index&quot;) (by default) . . Note: map() and apply() return new, transformed Series and DataFrames, respectively. They don&#8217;t modify the original data they&#8217;re called on. . review_points_mean = wines.points.mean() wines.points.map(lambda p: p - review_points_mean) . 0 -1.447138 1 -1.447138 2 -1.447138 ... 129968 1.552862 129969 1.552862 129970 1.552862 Name: points, Length: 129971, dtype: float64 . review_points_mean = wines.points.mean() wines.points - review_points_mean . 0 -1.447138 1 -1.447138 2 -1.447138 ... 129968 1.552862 129969 1.552862 129970 1.552862 Name: points, Length: 129971, dtype: float64 . def remean_points(row): row.points = row.points - review_points_mean return row wines.apply(remean_points, axis=&#39;columns&#39;) . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 0 Italy | Aromas include tropical fruit, broom, brimston... | Vulk√† Bianco | -1.447138 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Nicosia 2013 Vulk√† Bianco (Etna) | White Blend | Nicosia | . 1 Portugal | This is ripe and fruity, a wine that is smooth... | Avidagos | -1.447138 | 15.0 | Douro | NaN | NaN | Roger Voss | @vossroger | Quinta dos Avidagos 2011 Avidagos Red (Douro) | Portuguese Red | Quinta dos Avidagos | . 2 US | Tart and snappy, the flavors of lime flesh and... | NaN | -1.447138 | 14.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Rainstorm 2013 Pinot Gris (Willamette Valley) | Pinot Gris | Rainstorm | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 129968 France | Well-drained gravel soil gives this wine its c... | Kritt | 1.552862 | 30.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Gresser 2013 Kritt Gewurztraminer (Als... | Gew√ºrztraminer | Domaine Gresser | . 129969 France | A dry style of Pinot Gris, this is crisp with ... | NaN | 1.552862 | 32.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Marcel Deiss 2012 Pinot Gris (Alsace) | Pinot Gris | Domaine Marcel Deiss | . 129970 France | Big, rich and off-dry, this is powered by inte... | Lieu-dit Harth Cuv√©e Caroline | 1.552862 | 21.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Schoffit 2012 Lieu-dit Harth Cuv√©e Car... | Gew√ºrztraminer | Domaine Schoffit | . 129971 rows √ó 13 columns . # index of the bet bargain bidx = (wines.points / wines.price).idxmax() bargain_wine = wines.title.iloc[bidx] bargain_wine . &#39;Bandit NV Merlot (California)&#39; . def nb_stars(row): if row.country == &quot;Canada&quot;: return 3 elif row.points &gt;= 95: return 3 elif row.points &gt;= 85: return 2 else: return 1 star_ratings = wines.apply(nb_stars, axis=&quot;columns&quot;) print(star_ratings) . 0 2 1 2 2 2 .. 129968 2 129969 2 129970 2 Length: 129971, dtype: int64 . nb_tropical = sum(wines.description.map(lambda p: &quot;tropical&quot; in p)) nb_fruity = sum(wines.description.map(lambda p: &quot;fruity&quot; in p)) descriptor_counts = pd.Series([nb_tropical, nb_fruity], index=[&#39;tropical&#39;, &#39;fruity&#39;]) print(descriptor_counts) . tropical 3607 fruity 9090 dtype: int64 . GroupBy . wines.points.value_counts() . 88 17207 87 16933 90 15410 ... 98 77 99 33 100 19 Name: points, Length: 21, dtype: int64 . wines.groupby(&#39;points&#39;).points.count() . points 80 397 81 692 82 1836 ... 98 77 99 33 100 19 Name: points, Length: 21, dtype: int64 . wines.groupby(&#39;points&#39;).price.min() . points 80 5.0 81 5.0 82 4.0 ... 98 50.0 99 44.0 100 80.0 Name: price, Length: 21, dtype: float64 . wines.groupby(&#39;points&#39;).price.mean().round(1) . points 80 16.4 81 17.2 82 18.9 ... 98 245.5 99 284.2 100 485.9 Name: price, Length: 21, dtype: float64 . wines.groupby(&#39;winery&#39;).apply(lambda df: df.title.iloc[0]) . winery 1+1=3 1+1=3 NV Ros√© Sparkling (Cava) 10 Knots 10 Knots 2010 Viognier (Paso Robles) 100 Percent Wine 100 Percent Wine 2015 Moscato (California) ... √ñkonomierat Rebholz √ñkonomierat Rebholz 2007 Von Rotliegenden Sp√§t... √†Maurice √†Maurice 2013 Fred Estate Syrah (Walla Walla V... ≈†toka ≈†toka 2009 Izbrani Teran (Kras) Length: 16757, dtype: object . wines.groupby([&#39;country&#39;, &#39;province&#39;]).apply(lambda df: df.loc[df.points.idxmax()]) . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . country province . Argentina Mendoza Province Argentina | If the color doesn&#39;t tell the full story, the ... | Nicasia Vineyard | 97 | 120.0 | Mendoza Province | Mendoza | NaN | Michael Schachner | @wineschach | Bodega Catena Zapata 2006 Nicasia Vineyard Mal... | Malbec | Bodega Catena Zapata | . Other Argentina | Take note, this could be the best wine Colom√© ... | Reserva | 95 | 90.0 | Other | Salta | NaN | Michael Schachner | @wineschach | Colom√© 2010 Reserva Malbec (Salta) | Malbec | Colom√© | . Armenia Armenia Armenia | Deep salmon in color, this wine offers a bouqu... | Estate Bottled | 88 | 15.0 | Armenia | NaN | NaN | Mike DeSimone | @worldwineguys | Van Ardi 2015 Estate Bottled Ros√© (Armenia) | Ros√© | Van Ardi | . ... ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Uruguay Progreso Uruguay | Rusty in color but deep and complex in nature,... | Etxe Oneko Fortified Sweet Red | 90 | 46.0 | Progreso | NaN | NaN | Michael Schachner | @wineschach | Pisano 2007 Etxe Oneko Fortified Sweet Red Tan... | Tannat | Pisano | . San Jose Uruguay | Baked, sweet, heavy aromas turn earthy with ti... | El Preciado Gran Reserva | 87 | 50.0 | San Jose | NaN | NaN | Michael Schachner | @wineschach | Castillo Viejo 2005 El Preciado Gran Reserva R... | Red Blend | Castillo Viejo | . Uruguay Uruguay | Cherry and berry aromas are ripe, healthy and ... | Blend 002 Limited Edition | 91 | 22.0 | Uruguay | NaN | NaN | Michael Schachner | @wineschach | Narbona NV Blend 002 Limited Edition Tannat-Ca... | Tannat-Cabernet Franc | Narbona | . 425 rows √ó 13 columns . wines.groupby([&#39;country&#39;]).price.agg([len, min, max]) . len min max . country . Argentina 3800.0 | 4.0 | 230.0 | . Armenia 2.0 | 14.0 | 15.0 | . Australia 2329.0 | 5.0 | 850.0 | . ... ... | ... | ... | . US 54504.0 | 4.0 | 2013.0 | . Ukraine 14.0 | 6.0 | 13.0 | . Uruguay 109.0 | 10.0 | 130.0 | . 43 rows √ó 3 columns . countries_reviewed = wines.groupby([&#39;country&#39;, &#39;province&#39;]).description.agg([len]) countries_reviewed . len . country province . Argentina Mendoza Province 3264 | . Other 536 | . Armenia Armenia 2 | . ... ... ... | . Uruguay Progreso 11 | . San Jose 3 | . Uruguay 24 | . 425 rows √ó 1 columns . mi = countries_reviewed.index type(mi) . pandas.core.indexes.multi.MultiIndex . countries_reviewed.reset_index() . country province len . 0 Argentina | Mendoza Province | 3264 | . 1 Argentina | Other | 536 | . 2 Armenia | Armenia | 2 | . ... ... | ... | ... | . 422 Uruguay | Progreso | 11 | . 423 Uruguay | San Jose | 3 | . 424 Uruguay | Uruguay | 24 | . 425 rows √ó 3 columns . Sorting . myDataFrame.sort_values(by=&quot;nameColumn&quot;) # ascending=True by default . . myDataFrame.sort_values(by=&quot;nameColumn&quot;, ascending=False) . . myDataFrame.sort_values(by=[&quot;nameColumn1&quot;, &quot;nameColumn2&quot;]) . . myDataFrame.sort_index() . countries_reviewed = countries_reviewed.reset_index() countries_reviewed.sort_values(by=&#39;len&#39;) . country province len . 179 Greece | Muscat of Kefallonian | 1 | . 192 Greece | Sterea Ellada | 1 | . 194 Greece | Thraki | 1 | . ... ... | ... | ... | . 118 France | Bordeaux | 5941 | . 415 US | Washington | 8639 | . 392 US | California | 36247 | . 425 rows √ó 3 columns . countries_reviewed.sort_values(by=&#39;len&#39;, ascending=False) . country province len . 392 US | California | 36247 | . 415 US | Washington | 8639 | . 118 France | Bordeaux | 5941 | . ... ... | ... | ... | . 357 South Africa | Piekenierskloof | 1 | . 63 Chile | Coelemu | 1 | . 149 Greece | Beotia | 1 | . 425 rows √ó 3 columns . countries_reviewed.sort_index() . country province len . 0 Argentina | Mendoza Province | 3264 | . 1 Argentina | Other | 536 | . 2 Armenia | Armenia | 2 | . ... ... | ... | ... | . 422 Uruguay | Progreso | 11 | . 423 Uruguay | San Jose | 3 | . 424 Uruguay | Uruguay | 24 | . 425 rows √ó 3 columns . countries_reviewed.sort_values(by=[&#39;country&#39;, &#39;len&#39;]) . country province len . 1 Argentina | Other | 536 | . 0 Argentina | Mendoza Province | 3264 | . 2 Armenia | Armenia | 2 | . ... ... | ... | ... | . 420 Uruguay | Juanico | 12 | . 424 Uruguay | Uruguay | 24 | . 419 Uruguay | Canelones | 43 | . 425 rows √ó 3 columns . Data Types . myDataFrameOrSeries.nameColumn.dtype . . myDataFrameOrSeries.index.dtype . . myDataFrameOrSeries.dtypes . . myDataFrameOrSeries.nameColumn.astype(&quot;nameOfType&quot;) . wines . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 0 Italy | Aromas include tropical fruit, broom, brimston... | Vulk√† Bianco | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Nicosia 2013 Vulk√† Bianco (Etna) | White Blend | Nicosia | . 1 Portugal | This is ripe and fruity, a wine that is smooth... | Avidagos | 87 | 15.0 | Douro | NaN | NaN | Roger Voss | @vossroger | Quinta dos Avidagos 2011 Avidagos Red (Douro) | Portuguese Red | Quinta dos Avidagos | . 2 US | Tart and snappy, the flavors of lime flesh and... | NaN | 87 | 14.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Rainstorm 2013 Pinot Gris (Willamette Valley) | Pinot Gris | Rainstorm | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 129968 France | Well-drained gravel soil gives this wine its c... | Kritt | 90 | 30.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Gresser 2013 Kritt Gewurztraminer (Als... | Gew√ºrztraminer | Domaine Gresser | . 129969 France | A dry style of Pinot Gris, this is crisp with ... | NaN | 90 | 32.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Marcel Deiss 2012 Pinot Gris (Alsace) | Pinot Gris | Domaine Marcel Deiss | . 129970 France | Big, rich and off-dry, this is powered by inte... | Lieu-dit Harth Cuv√©e Caroline | 90 | 21.0 | Alsace | Alsace | NaN | Roger Voss | @vossroger | Domaine Schoffit 2012 Lieu-dit Harth Cuv√©e Car... | Gew√ºrztraminer | Domaine Schoffit | . 129971 rows √ó 13 columns . wines.index.dtype . dtype(&#39;int64&#39;) . wines.price.dtype . dtype(&#39;float64&#39;) . Missing Data . . Important: Entries missing values are given the value NaN, short for &quot;Not a Number&quot;. For technical reasons these NaN values are always of the float64 dtype. . myDataFrameOrSeries[pd.isnull(myDataFrameOrSeries.nameColumn)] . . myDataFrameOrSeries[pd.notnull(myDataFrameOrSeries.nameColumn)] . . myDataFrameOrSeries.nameColumn.fillna(value) . . myDataFrameOrSeries.nameColumn.replace(oldValue, newValue) . wines[pd.isnull(wines.country)] . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 913 NaN | Amber in color, this wine has aromas of peach ... | Asureti Valley | 87 | 30.0 | NaN | NaN | NaN | Mike DeSimone | @worldwineguys | Gotsa Family Wines 2014 Asureti Valley Chinuri | Chinuri | Gotsa Family Wines | . 3131 NaN | Soft, fruity and juicy, this is a pleasant, si... | Partager | 83 | NaN | NaN | NaN | NaN | Roger Voss | @vossroger | Barton &amp; Guestier NV Partager Red | Red Blend | Barton &amp; Guestier | . 4243 NaN | Violet-red in color, this semisweet wine has a... | Red Naturally Semi-Sweet | 88 | 18.0 | NaN | NaN | NaN | Mike DeSimone | @worldwineguys | Kakhetia Traditional Winemaking 2012 Red Natur... | Ojaleshi | Kakhetia Traditional Winemaking | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 129408 NaN | El Capricho is one of Uruguay&#39;s more consisten... | Reserve | 89 | 22.0 | NaN | NaN | NaN | Michael Schachner | @wineschach | El Capricho 2015 Reserve Tempranillo | Tempranillo | El Capricho | . 129590 NaN | A blend of 60% Syrah, 30% Cabernet Sauvignon a... | Shah | 90 | 30.0 | NaN | NaN | NaN | Mike DeSimone | @worldwineguys | B√ºy√ºl√ºbaƒü 2012 Shah Red | Red Blend | B√ºy√ºl√ºbaƒü | . 129900 NaN | This wine offers a delightful bouquet of black... | NaN | 91 | 32.0 | NaN | NaN | NaN | Mike DeSimone | @worldwineguys | Psagot 2014 Merlot | Merlot | Psagot | . 63 rows √ó 13 columns . countryWines = wines.country . countryWines . 0 Italy 1 Portugal 2 US ... 129968 France 129969 France 129970 France Name: country, Length: 129971, dtype: object . countryWines[pd.isnull] . 913 NaN 3131 NaN 4243 NaN ... 129408 NaN 129590 NaN 129900 NaN Name: country, Length: 63, dtype: object . countryWines.isnull() . 0 False 1 False 2 False ... 129968 False 129969 False 129970 False Name: country, Length: 129971, dtype: bool . countryWines = countryWines.fillna(&quot;Unknown&quot;) . countryWines[pd.isnull] . Series([], Name: country, dtype: object) . countryWines[913] . &#39;Unknown&#39; . countryWines = countryWines.replace(&quot;Unknown&quot;, &quot;Invalid&quot;) . countryWines[913] . &#39;Invalid&#39; . Renaming . myDataFrame.rename(columns={&quot;oldColumnName&quot;: &quot;newColumnName&quot;}) . . myDataFrame.rename(index={oldValueIndex: newValueIndex}) . . myDataFrame.rename_axis(&quot;nameOfIndex&quot;, axis=&#39;rows&#39;).rename_axis(&quot;nameOfFields&quot;, axis=&#39;columns&#39;) . winesTest = wines winesTest.head() . country description designation points price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 0 Italy | Aromas include tropical fruit, broom, brimston... | Vulk√† Bianco | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Nicosia 2013 Vulk√† Bianco (Etna) | White Blend | Nicosia | . 1 Portugal | This is ripe and fruity, a wine that is smooth... | Avidagos | 87 | 15.0 | Douro | NaN | NaN | Roger Voss | @vossroger | Quinta dos Avidagos 2011 Avidagos Red (Douro) | Portuguese Red | Quinta dos Avidagos | . 2 US | Tart and snappy, the flavors of lime flesh and... | NaN | 87 | 14.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Rainstorm 2013 Pinot Gris (Willamette Valley) | Pinot Gris | Rainstorm | . 3 US | Pineapple rind, lemon pith and orange blossom ... | Reserve Late Harvest | 87 | 13.0 | Michigan | Lake Michigan Shore | NaN | Alexander Peartree | NaN | St. Julian 2013 Reserve Late Harvest Riesling ... | Riesling | St. Julian | . 4 US | Much like the regular bottling from 2012, this... | Vintner&#39;s Reserve Wild Child Block | 87 | 65.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Sweet Cheeks 2012 Vintner&#39;s Reserve Wild Child... | Pinot Noir | Sweet Cheeks | . winesTest = winesTest.rename(columns={&quot;points&quot;: &quot;score&quot;}).head() . winesTest.head() . country description designation score price province region_1 region_2 taster_name taster_twitter_handle title variety winery . 0 Italy | Aromas include tropical fruit, broom, brimston... | Vulk√† Bianco | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Nicosia 2013 Vulk√† Bianco (Etna) | White Blend | Nicosia | . 1 Portugal | This is ripe and fruity, a wine that is smooth... | Avidagos | 87 | 15.0 | Douro | NaN | NaN | Roger Voss | @vossroger | Quinta dos Avidagos 2011 Avidagos Red (Douro) | Portuguese Red | Quinta dos Avidagos | . 2 US | Tart and snappy, the flavors of lime flesh and... | NaN | 87 | 14.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Rainstorm 2013 Pinot Gris (Willamette Valley) | Pinot Gris | Rainstorm | . 3 US | Pineapple rind, lemon pith and orange blossom ... | Reserve Late Harvest | 87 | 13.0 | Michigan | Lake Michigan Shore | NaN | Alexander Peartree | NaN | St. Julian 2013 Reserve Late Harvest Riesling ... | Riesling | St. Julian | . 4 US | Much like the regular bottling from 2012, this... | Vintner&#39;s Reserve Wild Child Block | 87 | 65.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Sweet Cheeks 2012 Vintner&#39;s Reserve Wild Child... | Pinot Noir | Sweet Cheeks | . winesTest.rename_axis(&quot;wineIndex&quot;, axis=&quot;rows&quot;) . country description designation score price province region_1 region_2 taster_name taster_twitter_handle title variety winery . wineIndex . 0 Italy | Aromas include tropical fruit, broom, brimston... | Vulk√† Bianco | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Nicosia 2013 Vulk√† Bianco (Etna) | White Blend | Nicosia | . 1 Portugal | This is ripe and fruity, a wine that is smooth... | Avidagos | 87 | 15.0 | Douro | NaN | NaN | Roger Voss | @vossroger | Quinta dos Avidagos 2011 Avidagos Red (Douro) | Portuguese Red | Quinta dos Avidagos | . 2 US | Tart and snappy, the flavors of lime flesh and... | NaN | 87 | 14.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Rainstorm 2013 Pinot Gris (Willamette Valley) | Pinot Gris | Rainstorm | . 3 US | Pineapple rind, lemon pith and orange blossom ... | Reserve Late Harvest | 87 | 13.0 | Michigan | Lake Michigan Shore | NaN | Alexander Peartree | NaN | St. Julian 2013 Reserve Late Harvest Riesling ... | Riesling | St. Julian | . 4 US | Much like the regular bottling from 2012, this... | Vintner&#39;s Reserve Wild Child Block | 87 | 65.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Sweet Cheeks 2012 Vintner&#39;s Reserve Wild Child... | Pinot Noir | Sweet Cheeks | . winesTest.rename_axis(&quot;wineIndex&quot;, axis=&quot;rows&quot;).rename_axis(&quot;fields&quot;, axis=&quot;columns&quot;) . fields country description designation score price province region_1 region_2 taster_name taster_twitter_handle title variety winery . wineIndex . 0 Italy | Aromas include tropical fruit, broom, brimston... | Vulk√† Bianco | 87 | NaN | Sicily &amp; Sardinia | Etna | NaN | Kerin O‚ÄôKeefe | @kerinokeefe | Nicosia 2013 Vulk√† Bianco (Etna) | White Blend | Nicosia | . 1 Portugal | This is ripe and fruity, a wine that is smooth... | Avidagos | 87 | 15.0 | Douro | NaN | NaN | Roger Voss | @vossroger | Quinta dos Avidagos 2011 Avidagos Red (Douro) | Portuguese Red | Quinta dos Avidagos | . 2 US | Tart and snappy, the flavors of lime flesh and... | NaN | 87 | 14.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Rainstorm 2013 Pinot Gris (Willamette Valley) | Pinot Gris | Rainstorm | . 3 US | Pineapple rind, lemon pith and orange blossom ... | Reserve Late Harvest | 87 | 13.0 | Michigan | Lake Michigan Shore | NaN | Alexander Peartree | NaN | St. Julian 2013 Reserve Late Harvest Riesling ... | Riesling | St. Julian | . 4 US | Much like the regular bottling from 2012, this... | Vintner&#39;s Reserve Wild Child Block | 87 | 65.0 | Oregon | Willamette Valley | Willamette Valley | Paul Gregutt | @paulgwine | Sweet Cheeks 2012 Vintner&#39;s Reserve Wild Child... | Pinot Noir | Sweet Cheeks | . Combining . When two files have the same fields (columns)‚Äã: . pd.concat([myDataFrameOrSeries1, myDataFrameOrSeries2]) . . When two files have the same index but not the same fields (columns)‚Äã: . myDataFrameOrSeriesLeft.join(myDataFrameOrSeriesRight, lsuffix=&quot;nameLeft&quot;, rsuffix=&quot;nameRight&quot;) .",
            "url": "https://cecilegallioz.github.io/blog/python/panda/2021/04/17/ReminderPanda.html",
            "relUrl": "/python/panda/2021/04/17/ReminderPanda.html",
            "date": " ‚Ä¢ Apr 17, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Reminder on Jupyter NoteBook",
            "content": "Commands . Command Execute . Double click or [ENTER] | Enter a cell | . [CTRL]+[ENTER] | Execute the contents of a text or code cell and exit | . [ESC] | Exit from a cell without executing it | . | | . [b] | Insert a cell below | . [a] | Insert a cell above (Above) | . [d]+[d] | Delete a cell without confirmation | . | | . [y] | Transform cell to code cell Python | . [m] | Transform cell to text Markdown | . | | . [s] | Save | . [CTRL]+[s] | Save | . Formatting . Command Execute . # Title1 | Level 1 Title | . ###### Title6 | Level 6 title | . | | . ** ** | Insert bold | . * * | Insert italic | . ~~ ~~ | Insert barred | . Backslash | Prevents Markdown from interpreting a character | . [ENTER] | Create a paragraph by inserting a new line | . A line . Ordered list 1. . Element1 | Element2 | Non ordered list - . Element1 | Element2 | . Tasks -[ ] . [x] completed task | [ ] unrealized task | .",
            "url": "https://cecilegallioz.github.io/blog/jupyter/2021/04/16/ReminderJupyter.html",
            "relUrl": "/jupyter/2021/04/16/ReminderJupyter.html",
            "date": " ‚Ä¢ Apr 16, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "Reminder on Python",
            "content": "Useful informations . Variables are not typed, while objects are. | . Variables address objects (strong typing). | . Variable names are best in camelCase (first letter in lower case and no numbers in first) and as meaningful as possible. | . In Python to identify a block of instructions, you need an identation of 4 spaces and not a tab | . Small immutable objects are sometimes stored as singletons (only one possible occurrence) | . Basic functions . Function Description Type argument(s) Output . help() | Help on a function | Function name | String | . type() | Type of a variable | All | Type | . len() | Length | All | Int | . print() | Print to screen | All | Exit screen | . input(&quot;Question Text&quot;) | Retrieves value entered by user | String | All | . Types of data . Type Example Immutable? Singleton? Notes . Integer | 1 | Immutable | Possible | No loss of accuracy | . Boolean | True or False | Immutable | Possible | No loss of accuracy | . Float | 1.2 | Immutable | Possible | Possible loss of accuracy | . Complex | 1+2j | Immutable | Possible | Possible loss of accuracy | . String | &quot;blah&quot; or &#39;blah&#39; | Immutable | Possible | | . Tuple | ( ,) or ( , ,) or , | Immutable | Possible | | . | | | | | . List | [ , ] | Mutable | | Stores only the references | . Dictionary | { : , : } | Mutable | | Unordered and unchangeable keys/values | . Set | { , } | Mutable | | Unordered and unique immutable keys | . Operators . Operator Name Notes . = | Op√©rateur d&#39;affectation | | | | . # | Comments | | | | . | | | | | . + | Addition or concatenation | Int + Int = Int | Int + Float = Float | Text + Text = Text | . - | Subtraction | Int - Int = Int | Int - Float = Float | | . * | Multiplication | Int * Int = Int | Int * Float = Float | | . / | Division | Int / Int = Float | Int / Float = Float | | . | | | | | . % | Modulo | Subtract from division | | | . // | Whole division | Rounded down | | | . ** | Power | idem pow(a,b) | | | . abs() | Absolute value | | | | . | | | | | . str() | Cast in String | str(Int) = String | | | . int() | Cast in Int | int(Float) = Int | | | . float() | Cast in Float | float(Int) = Float | | | . | | | | | . += | Simultaneous addition and affection | | | | . Logic operators . Sign Description . == | equal to | . != | different | . &lt; | strictly less than | . &lt;= | lower than or equal to | . | | . not() | not | . and or &amp; | and | . or or &#39;pipe&#39; | or | . ^ | or exlusive | . | | . element in myGroup | is the element in my group | . Strings (immutable) . Method Description Argument(s) Output . objectString.upper() | All capitalized | - | Text | . objectString.capitalize() | First letter uppercase, subsequent letters lowercase | - | Text | . objectString.strip() | Removes spaces before and after the string | - | Text | . objectString.replace(&quot;old&quot;, &quot;new&quot;) | Replaces all occurrences of old with new | Text, Text | Text | . objectString.find(&quot;look&quot;) | Returns the position of the first occurrence of the searched word or -1 | Text | Int | . | | | | . objectString.strip(&#39;,&#39;).split(&#39;,&#39;) | Split put each word in a list according to a fixed character (&quot;,&quot;) and thanks to strip the last &quot;,&quot; will be deleted | - | List | . &quot;,&quot;.join([&#39;abc&#39;, &#39;def&#39;]) | Makes a string from a list according to a fixed character (e.g. here &quot;,&quot;) | - | Text | . f string . f&quot;blah-blah-blah {function1(variable1)} blah-blah-blah {function2(variable2)} blah-blah-blah&quot; . age = 45 name = &quot;Paul&quot; gap = 10 . f&quot;In {gap} year(s), {name} will be {age + gap} year(s) old.&quot; . &#39;In 10 year(s), Paul will be 55 year(s) old.&#39; . List (mutable) . nameList = [&quot;String&quot;, number, otherList, variable] . The list does not store the objects but only the reference to the object. Its size will not vary according to the size of the referenced objects. . The index starts at 0. . Unlike text, they are modified by their methods =&gt; they are mutable. . Use Method Description . Index | myList[1] | Accesses the 2nd element of the list | . | myList[-1] | Go to the last element of the list | . | myList[1:3] | Accesses the 2nd and 3rd elements of the list | . | myList.index(element) | Returns the index of the element | . | | | . Classic | print(myList) | Displays the entire list | . | len(myList) | Returns the length of a list | . | | | . Sorting | myList.sort() | Changes myList into a sorted list instead | . | sorted(myList) | Copy and sort myList | . | myList.copy() | Shallow copy | . | | | . Add | myList.append(element) | Adds the element to the end of the list | . | myList.insert(index, element) | Add the element to the specified index and move the following elements by one index | . | myList.extend([otherList]) | Add the list otherList to the end of the first one (myList) | . | | | . Remove | myList.pop(index) | Delete and display the element with the index as argument or the last one | . | maListe.remove(element) | Deletes the first occurrence of an element | . | del myList[index] | Deletes the element at the specified index | . | | | . String | &quot; &quot;.join(myList) | Transforms a list into a string according to the &quot; &quot;, the opposite of split() | . | | | . Random | random.choice(myList) | choose an element from the list | . | random.choices(myList,3) | choose 3 elements in the list with throw-in | . | random.sample(myList,3) | choose 3 elements from the list without throw-in | . myList = [8, 5, 12, 4, 45, 7] . myList1 = [8, 5, 12, 4, 45, 7] myList2 = myList1 # The reference to the objects are copied! myList1[0] = 0 myList2 . [0, 5, 12, 4, 45, 7] . myList[1] = 5 sum(myList) = 81 max(myList) = 45 . myList = list(range(2)) . myList = [0, 1] . myList.append(2) . myList = [0, 1, 2] . myList.insert(1,&quot;one half&quot;) . myList = [0, &#39;one half&#39;, 1, 2] . myList = [3, 5, 7, 6, 4, 1, 0, 2] myList.sort() . myList = [0, 1, 2, 3, 4, 5, 6, 7] . Tuple (immutable) . (element,) or element, =&gt; tuple singleton (element, element) or (element, element,) &lt;= for writing long tuples on several lines or element, element or element, element, =&gt; multiple tuple . These are sequence objects. . They have the same properties as lists except that they are immutable =&gt; once created they cannot be modified. . (a, b) = [3, 4] . a = 3 b = 4 . a, b = b, a . a = 4 b = 3 . e = list(range(10)) # tuple unpacking : cut a list # with the * we indicate that the length is unknown # with the _ or &quot;ignored&quot; we indicate that we are not interested in the content of this variable x, *_, y = e . x = 0 y = 9 . a = [1, 2] b = [3, 4] z = zip(a,b) [i for i in z] . [(1, 3), (2, 4)] . Dictionarie (mutable - immutable keys) . myDictionary = {key1:value1, key2:value2} . age = {&quot;eve&quot;:30, &quot;ana&quot;:25, &quot;bob&quot;:35} listeTupleAge = [(&quot;hel&quot;,21), (&quot;jon&quot;,41)] dicoAge = dict(listeTupleAge) age.update(dicoAge) . {&#39;eve&#39;: 30, &#39;ana&#39;: 25, &#39;bob&#39;: 35, &#39;hel&#39;: 21, &#39;jon&#39;: 41} . age[&#39;eve&#39;] . 30 . age.keys() # the result is a view . dict_keys([&#39;eve&#39;, &#39;ana&#39;, &#39;bob&#39;, &#39;hel&#39;, &#39;jon&#39;]) . age.values() # the result is a view . dict_values([30, 25, 35, 21, 41]) . for name,year in age.items(): print(f&quot;{name.capitalize()} is {year} years old&quot;) . Eve is 30 years old Ana is 25 years old Bob is 35 years old Hel is 21 years old Jon is 41 years old . Set (mutable - unordered) . {key1, key2} . A set is essentially a hash table. Close to dictionaries, mutable, unordered, they store only unique keys and are optimized for membership testing. . a = [1, 2, 1, 20, 1] s = set(a) . {1, 2, 20} . s.add(10) . {1, 2, 10, 20} . s.update([1, 3, 30]) . {1, 2, 3, 10, 20, 30} . s.discard(30) . {1, 2, 3, 10, 20} . 30 in s . False . If . if condition: block of instructions elif condition: block of instructions else: block of instructions . In conditional expression . Variable = valueTrue if test else valueFalse . In a test : . - False : False, 0, None, [], {}, (), &quot;&quot; - True : everything else . userName = &quot;C&quot; if len(userName)==1: print(&quot;Hello single!&quot;) elif len(userName)&gt;0: print(&quot;Hello&quot;, userName) else: print(&quot;Hello World!&quot;) . Hello single! . y = &quot;Hello single!&quot; if (len(userName) == 1) else &quot;Hello World!&quot; print(y) . Hello single! . Range(n) . range(*start, end + 1, *step) range(0, 5, 1) idem range(0,5) idem range(5) =&gt; 0, 1, 2, 3, 4 . x = range(2, 7, 2) for n in x: print(n) . 2 4 6 . For . for element in myGroup: instructions for i in range(start, end + 1, step): instructions . With the continue keyword, you can indicate that you want to go directly to the next element of the loop. With the keyword break, we interrupt the loop. . Note: with the function exit(), we can stop the programme . myList = [0, 3, 7, 13] for element in myList: print(element) . 0 3 7 13 . myText = &quot;World&quot; for element in myText: print(element) . W o r l d . While . while condition: instructions continue # ignore the following statements for this iteration break # exit while while True: # infinite loop instructions break . a = list(range(1,7)) print(a) while a: a.pop() if len(a) == 5: continue # ignore les instructions suivantes pour cette it√©ration print(a) . [1, 2, 3, 4, 5, 6] [1, 2, 3, 4] [1, 2, 3] [1, 2] [1] [] . Class . class ClassName(): &quot;&quot;&quot; documentation on several lines &quot;&quot;&quot; def __init__(self, p_argument1): self.argument1 = p_argument1 (...) def __str__(self): return &quot;argument1 = &quot; + str(self.argument1) . . Important: a class is the mold of an object, it represents the essence of a concept or a physical object. It defines both the data characterizing this object and the actions (functions) that this object performs. . Note: a class (the mold) is instancied = an object is created according to the mold . class Ship: def __init__(self, model, hp, armor, damage): self.model = model self.hp = hp self.armor = armor self.damage = damage def __str__(self): return self.model + &quot;, hp:&quot; + str(self.hp) + &quot;, armor:&quot; + str( self.armor) + &quot;, damage:&quot; + str(self.damage) # Creation of an instance of this class ship1 = Ship(&quot;CRUISER&quot;, 1200, 100, 100) # Print out this instance print(&quot;ship1: &quot;, ship1) # Creation of an instance of this class ship2 = Ship(&quot;FIGHTER&quot;, 300, 10, 1000) # Print out this instance print(&quot;ship2: &quot;, ship2) . ship1: CRUISER, hp:1200, armor:100, damage:100 ship2: FIGHTER, hp:300, armor:10, damage:1000 . Functions . def functionName(arguments): &quot;&quot;&quot; documentation on several lines &quot;&quot;&quot; instructions (with or without return or just pass) . Document well if the function modifies the parameter passed to it . def calculPerimetreTriangle (cote1, cote2, cote3): &quot;&quot;&quot; receives three side lengths and returns the sum of the three &quot;&quot;&quot; perimetre = cote1 + cote2 + cote3 return perimetre print(calculPerimetreTriangle(1,2,3)) . 6 . You can put an optional argument with default value (not a mutable object) . def functionName(argument1, argument2, argument3=defaultvalue3): instructions . def f(a, b=10): print(a, b) f(1) f(1, 2) . 1 10 1 2 . Tuple or dictionaries in arguments . To not have to specify the number of elements sent to the function: . #Tuple def f(*t): print(t) #dictionary def f(**d): print(d) . def f(*t): print(t) f(1) f(1, 2, 3) . (1,) (1, 2, 3) . def f(a, b): print(a, b) L = [1, 2] f(*L) . 1 2 . def f(**d): print(d) f(nom=&quot;Dupond&quot;, prenom=&quot;Paul&quot;) . {&#39;nom&#39;: &#39;Dupond&#39;, &#39;prenom&#39;: &#39;Paul&#39;} . Named arguments . When you call the function, you can pass the arguments in the order you want, if you name them: . functionName(nameArgument2=valueArgument2, nameArgument1=valueArgument1) . . Warning: In the function call, it is not possible to enter a positional argument after an argument named . def f(a, b): print(&quot;a =&quot;, a,&quot;b =&quot;, b) f(b=1, a=5) # but f(1, 5) # warning #f(b=1, 5) =&gt; SyntaxError: positional argument follows keyword argument . a = 5 b = 1 a = 1 b = 5 . Order of consumption of arguments . The 4 forms of arguments will be summarized, depending on how the arguments of the function have been written and how they are called : . all &quot;normal&quot; arguments, called positional | form *args which catches in a tuple the rest of the positional arguments | named arguments name=&lt;value&gt; | form **dargs which catches in a dictionary the remainder of the named arguments | def foo(positionalArg1, namedArg2=100, *tuplePositionalArg, **dictNamedArg): print(f&quot;positionalArg1={positionalArg1}, namedArg2={namedArg2}, *tuplePositionalArg={tuplePositionalArg}, **dictNamedArg={dictNamedArg}&quot;) . foo(1) . positionalArg1=1, namedArg2=100, *tuplePositionalArg=(), **dictNamedArg={} . foo(1, 2) . positionalArg1=1, namedArg2=2, *tuplePositionalArg=(), **dictNamedArg={} . foo(1, 2, 3) . positionalArg1=1, namedArg2=2, *tuplePositionalArg=(3,), **dictNamedArg={} . foo(1, 2, 3, 4, arg5=5, arg6=6) . positionalArg1=1, namedArg2=2, *tuplePositionalArg=(3, 4), **dictNamedArg={&#39;arg5&#39;: 5, &#39;arg6&#39;: 6} . def bar(positionalArg1, *tuplePositionalArg, namedArg2=100, **dictNamedArg): print(f&quot;positionalArg1={positionalArg1}, namedArg2={namedArg2}, *tuplePositionalArg={tuplePositionalArg}, **dictNamedArg={dictNamedArg}&quot;) . bar(1) . positionalArg1=1, namedArg2=100, *tuplePositionalArg=(), **dictNamedArg={} . bar(1, 2) . positionalArg1=1, namedArg2=100, *tuplePositionalArg=(2,), **dictNamedArg={} . bar(1, 2, namedArg2=3) . positionalArg1=1, namedArg2=3, *tuplePositionalArg=(2,), **dictNamedArg={} . bar(1, 2, 3, namedArg2=4, namedArg5=5) . positionalArg1=1, namedArg2=4, *tuplePositionalArg=(2, 3), **dictNamedArg={&#39;namedArg5&#39;: 5} . Lambda functions . Functions that we do not intend to reuse . lambda x: [function of x] . You can use a lambda to access a characteristic of an object for a sort . myList.sort(key=lambda x: x.characteristic1) . You can execute a lambda function on a list of arguments (x) with a map and see the results in a list with a list: . list(map(lambda x: [function of x], [list of x])) . and filter the results with a filter: . list(filter(lambda x: [test on [function of x]], [list of x])) . list(map(lambda x: x**2, range(10))) . [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] . list(filter(lambda x: x % 2 == 0, range(10))) . [0, 2, 4, 6, 8] . list(map(lambda x: x**2, filter(lambda x: x % 2 == 0, range(10)))) . [0, 4, 16, 36, 64] . List comprehensions = For on lists . We create a list, set or dictionary according to our needs. . on l a list . comprehesion_l = [ function(i) for i in l if test(i) ] . on s a set . comprehesion_s = { function(i) for i in s if test(i) } . on d a dictionary . comprehesion_d = { function(i): j for i,j in d.items() if test(i, j) } . a = [1, 4, 6, 9, 13] b = [i**2 for i in a] . [1, 16, 36, 81, 169] . name = [&#39;Alice&#39;, &#39;evE&#39;, &#39;sonia&#39;, &#39;BOB&#39;] name = [p.title() for p in name] . [&#39;Alice&#39;, &#39;Eve&#39;, &#39;Sonia&#39;, &#39;Bob&#39;] . Iterator . Generator expression . iterableI = (function(x) for x in list) . Simple and compact object that allows to browse an iterable object. . The iterable object contains the data, the iterator object contains the mechanism to traverse it. . They can be browsed only once. We recognize an iterator because it is its own iterator : . i is iter(i) =&gt; True . They have two methods: . * __iter()__ * __next()__ . square = [x**2 for x in range(1_000)] . 227 ¬µs ¬± 12.2 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each) . square2 = (x**2 for x in range(1_000)) . 515 ns ¬± 38.4 ns per loop (mean ¬± std. dev. of 7 runs, 1000000 loops each) . palindrome = (x for x in square2 if str(x) == str(x)[::-1]) list(palindrome) . [0, 1, 4, 9, 121, 484, 676, 10201, 12321, 14641, 40804, 44944, 69696, 94249, 698896] . generator = (x**2 for x in range(10**18) if x%17==0) recherche = set() for x in generator: if x &gt; 10**10: break elif str(x)[-4:] == &#39;1316&#39;: # with a generator, you only pay for what you use recherche.add(x) . {617721316, 4536561316, 3617541316, 311381316} . Generator function . def functionName (arguments): instructions yield valueReturn . Function that returns an iterable so no memory space is used . def gen(it): for i in it: if isinstance(i, int): yield i**2 else: yield &#39;nan&#39; L = [1, 2, 0, &#39;18&#39;, &#39;x&#39;, [11], 25] . [1, 4, 0, &#39;nan&#39;, &#39;nan&#39;, &#39;nan&#39;, 625] . Modules and packages . A module is a Python file (.py), a package is a set of Python files (+ init.py). . The namespace is an attribute of the module. . Packages and modules are imported in the same way at the top of NoteBook . import moduleName as md import packageName as pk md.variableName md.functionName md.className pk.filename.variableName =&gt; perfect isolation of namespaces . Import with from (more complex) : . from nomModule import nomVariable =&gt; risk of collision between the local variable nameVariable and the imported one =&gt; no access to other attributes of moduleName . Package random . Method Use . random.random() | a float between 0 and 1 (1 not included) | . random.uniform(a, b) | a float between a and b (b not included) | . random.randint(a, b) | an integer between a and b (included) | random.gauss(average) | . import random print(random.random()) print(random.uniform(1,2)) print(random.randint(1,3)) . 0.11548597942353367 1.134093036662287 3 . import random population = range(1000) # Number of tickets desired N = 10 random.sample(population,N) . [932, 62, 122, 223, 623, 970, 217, 261, 535, 156] . def div(a,b): try: print(a/b) except ZeroDivisionError: print(&quot;oops, division by 0&quot;) except TypeError: print(&quot;oops, integers needed&quot;) . div(5,2) . 2.5 . div(5,0) . oops, division by 0 . div(&quot;cinq&quot;,2) . oops, integers needed . Exceptions . try: expression0 except nomException1: expression1 except nomException2: expression2 else: # on passe si aucune exception est attrap√©e (on n&#39;y passe pas s&#39;il y a un return avant) expression3 finally: # on passe dans le finally m√™me s&#39;il y a un return avant expression4 . Mistrust on the floats . 0.3 - 0.1 . 0.19999999999999998 . 0.3 - 0.1 == 0.2 . False . from decimal import Decimal Decimal(&#39;0.3&#39;) - Decimal(&#39;0.1&#39;) == Decimal(&#39;0.2&#39;) . True . from fractions import Fraction Fraction(3, 10) - Fraction(1, 10) == Fraction(2, 10) . True . Time mesurement . import datetime start_time = datetime.datetime.now() print(f&quot;Start: {start_time} microseconds&quot;) . Start: 2021-05-18 19:12:12.206309 microseconds . print(f&quot;Delay: {datetime.datetime.now() - start_time} microseconds&quot;) . Delay: 0:00:02.790415 microseconds . Choosing a priority after going through an entire list . priority = 100 choice = None for l in myList: if tests1 and priority &gt; 1: choice = &quot;1&quot; priority = 1 if test2 and priority &gt; 2: choice = &quot;2&quot; priority = 2 print(choice) . myList = [5, 0, 10, 7, 11, 1, 12] priority = 100 choice = None for l in myList: if l &gt; 10 and priority &gt; 1: choice = l priority = 1 if l &gt; 2 and priority &gt; 2: choice = l priority = 2 print(choice) . 11 .",
            "url": "https://cecilegallioz.github.io/blog/python/2021/04/15/ReminderPython.html",
            "relUrl": "/python/2021/04/15/ReminderPython.html",
            "date": " ‚Ä¢ Apr 15, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "My sources",
            "content": "What I‚Äôm using in my journey ? . Logo Source . | Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD, authors : Howard, J. and Gugger, S., publisher O‚ÄôReilly Media, Incorporated | . | Kaggle/Courses | . | Python 3 : des fondamentaux aux concepts avanc√©s du langage | . | D√©marrez votre projet avec Python | .",
            "url": "https://cecilegallioz.github.io/blog/general/2021/04/14/Sources.html",
            "relUrl": "/general/2021/04/14/Sources.html",
            "date": " ‚Ä¢ Apr 14, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is C√©cile Gallioz, I live in France with my husband and son. . After 15 years in business intelligence and requirements gathering, I am adding a string to my bow with Python development. . My goal is to develop tools to improve productivity and efficiency. . My values are creativity, transmission and integrity. . I have a master‚Äôs degree in mathematical engineering and a bachelor‚Äôs degree in computer development. . My profile on Kaggle . My profile on LinkedIn . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://cecilegallioz.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://cecilegallioz.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}